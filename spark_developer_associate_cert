# Exam Pattern --

Apache Spark Architecture and Components - 20%

Using Spark SQL - 20%

Developing Apache Spark‚Ñ¢ DataFrame/DataSet API Applications - 30%

Troubleshooting and Tuning Apache Spark DataFrame API Applications - 10%

Structured Streaming - 10%

Using Spark Connect to deploy applications - 5%

Using Pandas API on Apache Spark - 5%

# Exam syllabus --

Exam Outline
-------------------------------------------------------------------------------

Section 1: Apache Spark Architecture and Components
-------------------------------------------------------------------------------

‚óè Identify the advantages and challenges of implementing Spark.

‚óè Identify the role of core components of Apache Spark‚Ñ¢'s Architecture, including cluster,
driver node, worker nodes/executors, CPU cores, and memory.

‚óè Describe the architecture of Apache Spark‚Ñ¢, including DataFrame and Dataset concepts,
SparkSession lifecycle, caching, storage levels, and garbage collection.

‚óè Explain the Apache Spark‚Ñ¢ Architecture execution hierarchy..

‚óè Configure Spark partitioning in distributed data processing, including shuffles and partitions

‚óè Describe the execution patterns of the Apache Spark‚Ñ¢ engine, including actions,
transformations, and lazy evaluation.

‚óè Identify the features of the Apache Spark Modules, including Core, Spark SQL, DataFrames,
Pandas API on Spark, Structured Streaming, and MLib.


Section 2: Using Spark SQL
-------------------------------------------------------------------------------

‚óè Utilize common data sources such as JDBC, files, etc., to efficiently read from and write to
Spark DataFrames using Spark SQL, including overwriting and partitioning by column.

‚óè Execute SQL queries directly on files, including ORC Files, JSON Files, CSV Files, Text Files,
and Delta Files, and understand the different save modes for outputting data in Spark SQL.

‚óè Save data to persistent tables while applying sorting and partitioning to optimize data
retrieval.

‚óè Register DataFrames as temporary views in Spark SQL, allowing them to be queried with
SQL syntax.


Section 3: Developing Apache Spark‚Ñ¢ DataFrame/DataSet API Applications
-------------------------------------------------------------------------------

‚óè Manipulate columns, rows, and table structures by adding, dropping, splitting, renaming
column names, applying filters, and exploding arrays.

‚óè Perform data deduplication and validation operations on DataFrames.

‚óè Perform aggregate operations on DataFrames such as count, approximate count distinct,
and mean, summary.

‚óè Manipulate and utilize Date data type, such as Unix epoch to date string, and extract date
component.

‚óè Combine DataFrames with operations such as Inner join, left join, broadcast join, multiple
keys, cross join, union, and union all.

‚óè Manage input and output operations by writing, overwriting, and reading DataFrames with
schemas.

‚óè Perform operations on DataFrames such as sorting, iterating, printing schema, and
conversion between DataFrame and sequence/list formats.

‚óè Create and invoke user-defined functions with or without stateful operators, including
StateStores.

‚óè Describe different types of variables in Spark, including broadcast variables and
accumulators.

‚óè Describe the purpose and implementation of broadcast joins


Section 4: Troubleshooting and Tuning Apache Spark DataFrame API Applications.
-------------------------------------------------------------------------------

‚óè Implement performance tuning strategies & optimize cluster utilization, including
partitioning, repartitioning, coalescing, identifying data skew, and reducing shuffling

‚óè Describe Adaptive Query Execution (AQE) and its benefits.

‚óè Perform logging and monitoring of Spark applications - publish, customize, and analyze
Driver logs and Executor logs to diagnose out-of-memory errors, cluster underutilization,
etc.


Section 5: Structured Streaming
-------------------------------------------------------------------------------

‚óè Explain the Structured Streaming engine in Spark, including its functions, programming
model, micro-batch processing, exactly-once semantics, and fault tolerance mechanisms.

‚óè Create and write Streaming DataFrames and Streaming Datasets, including the basic output
modes and output sinks.

‚óè Perform basic operations on Streaming DataFrames and Streaming Datasets, such as
selection, projection, window and aggregation.

‚óè Perform Streaming Deduplication in Structured Streaming, both with and without watermark
usage.


Section 6: Using Spark Connect to deploy applications
-------------------------------------------------------------------------------

‚óè Describe the features of Spark Connect.

‚óè Describe the different deployment mode types (Client, Cluster, Local) in the Apache Spark‚Ñ¢
environment.


Section 7: Using Pandas API on Spark
-------------------------------------------------------------------------------

‚óè Explain the advantages of using Pandas API on Spark.

‚óè Create and invoke Pandas UDF


=================================================================================================================
=================================================================================================================

DATABRICKS SAMPLE QUESTIONS


Question 1

Objective - Utilize common data sources such as JDBC, files, etc. to efficiently read from
and write to Spark DataFrames using SparkSQL, including overwrite and partitioning by
column.

A data engineer needs to write a DataFrame df to a Parquet file, partitioned by the column country,
and overwrite any existing data at the destination path.
Which code should the data engineer use to accomplish this task in Apache Spark?
A. df.write.mode("append").partitionBy("country").parquet("/data/output")
B. df.write.partitionBy("country").parquet("/data/output")
C. df.write.mode("overwrite").parquet("/data/output")
D.
df.write.mode("overwrite").partitionBy("country").parquet("/data/output")


Ans - D

Question 2

Objective - Perform logging and monitoring of Spark applications - publish, customize, and
analyze Driver logs and Executor logs to diagnose out-of-memory errors, cluster
underutilization, etc.

An engineer notices a significant increase in the job execution time during the execution of a Spark
job. After some investigation, the engineer decides to check the logs produced by the Executors.
How should the engineer retrieve the Executor logs to diagnose performance issues in the Spark
application?

A. Use the command 'spark-submit' with the '--verbose' flag to print the logs to the console.
B. Locate the executor logs on the Spark master node, typically under the /tmp directory.
C. Use the Spark UI to select the stage and view the executor logs directly from the stages tab.
D. Fetch the logs by running a Spark job with the `spark-sql` CLI tool.

Ans - C

Question 3:

Objective: Manipulate columns, rows, and table structures by adding, dropping, splitting, renaming
column names, applying filters, and exploding arrays.

Which code block will replace the division column in the storesDF DataFrame with a new column
named state, and simultaneously replace and rename the mName column to managerName in the
resulting DataFrame?

A. storesDF = (storesDF.withColumn("state", col("division"))
.drop("division")
.withColumnRenamed("mName", "managerName"))
B. storesDF = (storesDF.withColumnRenamed("state", "division")
.withColumnRenamed("mName", "managerName"))
C. storesDF = (storesDF.withColumn(col("division"), state)
.withColumnRenamed("managerName", "mName"))
D. storesDF = (storesDF .drop("division")
.withColumnRenamed("state", lit("default_state"))
.withColumnRenamed(columns={"mName": "managerName"}))

Ans - A

Question 4:

Objective: Perform operations on DataFrames such as sorting, iterating, printing schema, and
conversion between DataFrame and sequence/list formats.

Given a DataFrame employeeDF with columns: name, department, salary, and age.
Which code snippet sorts a DataFrame by multiple columns in descending order, printing its
schema, and converting the DataFrame to a list of rows?

A. result = employeeDF.orderBy(desc("salary"), desc("age"))
employeeDF.printSchema()
row_list = result.toPandas().values.tolist()
B. result = employeeDF.sort("salary", "age", ascending=[False,
False]) employeeDF.schema()
row_list = result.toList()
C. result = (employeeDF .orderBy(col("salary").desc(),
col("age").desc()) .collect())
employeeDF.printSchema()
row_list = [row.asDict() for row in result]
D. result = (employeeDF
.sort(["salary", "age"], descending=True)
.show())
employeeDF.describe()
row_list = list(result)

Ans - C

Question 5:

Objective: Configure Spark partitioning in distributed data processing, including shuffles and
partitions.

What will be the impact of setting the default value of spark.sql.shuffle.partitions to 200?

A. DataFrames will be divided into 200 distinct partitions during data shuffling operations.
B. New DataFrames created by Spark will be partitioned to utilize the memory of 200 executors
optimally.
C. All DataFrames in Spark will be partitioned to occupy the memory of 200 executors perfectly.
D. Spark will only process the first 200 partitions of DataFrames to enhance performance.

Ans - A


Question 6:

Objective: Perform data deduplication and validation operations on DataFrames.

Which code fragment will return a new DataFrame from storesDF that excludes all rows
containing at least one missing value in any column?

A. storesDF.na.drop("all")
B. storesDF.na.drop(subset = "sqft")
C. storesDF.na.drop()
D. storesDF.dropna("all")

Ans - C


Question 7:

Objective: Describe the different deployment mode types (Client, Cluster, Local) in Apache Spark
environment.

Which Spark deployment mode requires all executors to run on a single worker node?

A. Cluster mode
B. Local mode
C. Client mode
D. Standard mode

Ans - B 

Question 8:

Objective: Explain the Structured Streaming engine in Spark, including its functions, programming
model, micro-batch processing, exactly-once semantics, and fault tolerance mechanisms.

A developer must calculate real-time, rolling metrics like "average session duration in the last hour"
and "top 10 products viewed in the last 15 minutes" from continuous clickstream data for
recommendation engines. These metrics must update every 2 minutes. They chose Streaming
DataFrames over traditional batch DataFrames.

Why should the developer consider Streaming DataFrames in this use case?

A. Streaming DataFrames automatically handle data partitioning and caching more efficiently than
batch DataFrames, making them faster for processing large volumes of clickstream data, even
when processing historical data.
B. Streaming DataFrames enable continuous data processing with incremental updates to
aggregations, allowing real-time metrics without reprocessing the entire dataset every 2 minutes.
C. Streaming DataFrames provide better error handling and automatic retry mechanisms than
batch DataFrames, which is essential for handling unreliable network connections from the network
recovery mechanism.
D. Streaming DataFrames use more memory than batch DataFrames because they process data in
batches, making them ideal for this scenario.

Ans - B

Question 9:

Objective: Create and invoke user-defined functions with or without stateful operators, including
StateStores.

A developer is building a streaming application. The application's validation logic involves
computations and string manipulations unavailable in built-in Spark functions. The developer
wants to ensure the streaming job is stateless for each transaction and can scale horizontally
without maintaining customer history.

When should the developer use user-defined functions (UDFs) without stateful operators for this
transaction validation pipeline?

A. Maintain running totals and compare them to historical averages in state.
B. Perform windowed aggregations over the last 24 hours with incremental updates.
C. Detect session-based patterns by tracking sequences across events.
D. Apply custom logic without relying on previous transaction data or state between transaction

Ans - D


Question 10:

Objective: Perform aggregate operations on DataFrames such as count, approximate count
distinct, and mean, summary.

A developer needs a dashboard for a mobile app's 50 million user activity records, showing total
event and unique user counts by time. The dashboard refreshes hourly, and management
prioritizes speed and accepts a 2-3% error margin for unique user counts.

Why should the developer choose approx_count_distinct() over count(distinct())?

A. approx_count_distinct() offers improved accuracy over count(distinct()) for large
datasets due to its advanced statistical algorithms.
B. approx_count_distinct() handles nulls and missing data better than
count(distinct()), which is crucial for incomplete user activity logs.
C. approx_count_distinct() uses probabilistic algorithms (HyperLogLog) to boost
performance by avoiding costly shuffle operations.
D. approx_count_distinct() uses less memory per partition than count(distinct())
because it stores aggregated results in compressed format,

Ans - C 



---------------------------------------------------------------------------------------
UDEMY TEST - 1 :

Question 1 --

A data engineer needs to write a DataFrame df to a Parquet file, partitioned by the column country, and overwrite any existing data at the destination path.

Which code should the data engineer use to accomplish this task in Apache Spark?

Adf.write.mode('overwrite').partitionBy('country').parquet('/data/output')
Bdf.write.mode('append').partitionBy('country').parquet('/data/output')
Cdf.write.mode('overwrite').parquet('/data/output')
Ddf.write.partitionBy('country').parquet('/data/output')


Answer : A

The .mode('overwrite') ensures that existing files at the path will be replaced.

.partitionBy('country') optimizes queries by writing data into partitioned folders.

Correct syntax:

df.write.mode('overwrite').partitionBy('country').parquet('/data/output')

--- Source: Spark SQL, DataFrames and Datasets Guide



Question 2 --

A data engineer is reviewing a Spark application that applies several transformations to a DataFrame but notices that the job does not start executing immediately.

Which two characteristics of Apache Spark's execution model explain this behavior?

Choose 2 answers:

A. The Spark engine requires manual intervention to start executing transformations.
B. Only actions trigger the execution of the transformation pipeline.
C. Transformations are executed immediately to build the lineage graph.
D. The Spark engine optimizes the execution plan during the transformations, causing delays.
E. Transformations are evaluated lazily.

Ans - B,E


Question 3
Which command overwrites an existing JSON file when writing a DataFrame?

A. df.write.mode('overwrite').json('path/to/file')
B. df.write.overwrite.json('path/to/file')
C. df.write.json('path/to/file', overwrite=True)
D. df.write.format('json').save('path/to/file', mode='overwrite')
Ans -- A 

Question 4
What is the benefit of using Pandas on Spark for data transformations?

Options:

A. It is available only with Python, thereby reducing the learning curve.
B. It computes results immediately using eager execution, making it simple to use.
C. It runs on a single node only, utilizing the memory with memory-bound DataFrames and hence cost-efficient.
D. It executes queries faster using all the available cores in the cluster as well as provides Pandas's rich set of features.


Answer : D

Pandas API on Spark (formerly Koalas) offers:

Familiar Pandas-like syntax

Distributed execution using Spark under the hood

Scalability for large datasets across the cluster

It provides the power of Spark while retaining the productivity of Pandas.


Question 5
A data scientist is working with a Spark DataFrame called customerDF that contains customer information. The DataFrame has a column named email with customer email addresses. The data scientist needs to split this column into username and domain parts.

Which code snippet splits the email column into username and domain columns?

A.

customerDF.select(

col("email").substr(0, 5).alias("username"),

col("email").substr(-5).alias("domain")

)

B.

customerDF.withColumn("username", split(col("email"), "@").getItem(0)) \

.withColumn("domain", split(col("email"), "@").getItem(1))

C.

customerDF.withColumn("username", substring_index(col("email"), "@", 1)) \

.withColumn("domain", substring_index(col("email"), "@", -1))

D.

customerDF.select(

regexp_replace(col("email"), "@", "").alias("username"),

regexp_replace(col("email"), "@", "").alias("domain")

)

AOption A
BOption B
COption C
DOption D


Answer : B

Option B is the correct and idiomatic approach in PySpark to split a string column (like email) based on a delimiter such as '@'.

The split(col('email'), '@') function returns an array with two elements: username and domain.

getItem(0) retrieves the first part (username).

getItem(1) retrieves the second part (domain).

withColumn() is used to create new columns from the extracted values.

Example from official Databricks Spark documentation on splitting columns:

from pyspark.sql.functions import split, col

df.withColumn('username', split(col('email'), '@').getItem(0)) \

.withColumn('domain', split(col('email'), '@').getItem(1))

Why other options are incorrect:

A uses fixed substring indices (substr(0, 5)), which won't correctly extract usernames and domains of varying lengths.

C uses substring_index, which is available but less idiomatic for splitting emails and is slightly less readable.

D removes '@' from the email entirely, losing the separation between username and domain, and ends up duplicating values in both fields.

Therefore, Option B is the most accurate and reliable solution according to Apache Spark 3.5 best practices.



Question 6
Which feature of Spark Connect is considered when designing an application to enable remote interaction with the Spark cluster?

A. It provides a way to run Spark applications remotely in any programming language
B. It can be used to interact with any remote cluster using the REST API
C. It allows for remote execution of Spark jobs
D. It is primarily used for data ingestion into Spark from external sources


Answer : C

Spark Connect introduces a decoupled client-server architecture. Its key feature is enabling Spark job submission and execution from remote clients --- in Python, Java, etc.

From Databricks documentation:

''Spark Connect allows remote clients to connect to a Spark cluster and execute Spark jobs without being co-located with the Spark driver.''

A is close, but 'any language' is overstated (currently supports Python, Java, etc., not literally all).

B refers to REST, which is not Spark Connect's mechanism.

D is incorrect; Spark Connect isn't focused on ingestion.

Final Answer: C



Question 7
A developer runs:

df.partitionBy("country","colour")

What is the result?

Options:

A. It stores all data in a single Parquet file.
B. It throws an error if there are null values in either partition column.
C. It appends new partitions to an existing Parquet file.
D. It creates separate directories for each unique combination of color and fruit.


Answer : D

The partitionBy() method in Spark organizes output into subdirectories based on unique combinations of the specified columns:

e.g.

/path/to/output/color=red/fruit=apple/part-0000.parquet

/path/to/output/color=green/fruit=banana/part-0001.parquet

This improves query performance via partition pruning.

It does not consolidate into a single file.

Null values are allowed in partitions.

It does not 'append' unless .mode('append') is used.



--------------------------------------------------------------------
EXAMTOPICS Ques --

Question #1Topic 1
Which of the following describes the Spark driver?

A. The Spark driver is responsible for performing all execution in all execution modes ‚Äì it is the entire Spark application.
B. The Spare driver is fault tolerant ‚Äì if it fails, it will recover the entire Spark application.
C. The Spark driver is the coarsest level of the Spark execution hierarchy ‚Äì it is synonymous with the Spark application.
D. The Spark driver is the program space in which the Spark application‚Äôs main method runs coordinating the Spark entire application. Most Voted
E. The Spark driver is horizontally scaled to increase overall processing throughput of a Spark application.
 
Correct Answer: D 


Question #2Topic 1
Which of the following describes the relationship between nodes and executors?

A. Executors and nodes are not related.
B. Anode is a processing engine running on an executor.
C. An executor is a processing engine running on a node. Most Voted
D. There are always the same number of executors and nodes.
E. There are always more nodes than executors.
 
Correct Answer: C üó≥Ô∏è


Question #3Topic 1
Which of the following will occur if there are more slots than there are tasks?

A. The Spark job will likely not run as efficiently as possible. Most Voted
B. The Spark application will fail ‚Äì there must be at least as many tasks as there are slots.
C. Some executors will shut down and allocate all slots on larger executors first.
D. More tasks will be automatically generated to ensure all slots are being used.
E. The Spark job will use just one single slot to perform all tasks.
 
Correct Answer: A üó≥Ô∏è
Community vote distribution


Question #4Topic 1
Which of the following is the most granular level of the Spark execution hierarchy?

A. Task       
B. Executor
C. Node
D. Job
E. Slot
 
Correct Answer: A 


Question #5Topic 1
Which of the following statements about Spark jobs is incorrect?

A. Jobs are broken down into stages.
B. There are multiple tasks within a single job when a DataFrame has more than one partition.
C. Jobs are collections of tasks that are divided up based on when an action is called.
D. There is no way to monitor the progress of a job. Most Voted
E. Jobs are collections of tasks that are divided based on when language variables are defined.
 
Correct Answer: D üó≥Ô∏è
Community vote distribution
D (100%)


Question #6Topic 1
Which of the following operations is most likely to result in a shuffle?

A. DataFrame.join() 
B. DataFrame.filter()
C. DataFrame.union()
D. DataFrame.where()
E. DataFrame.drop()
 
Correct Answer: A 
-- Join, GroupBy, Distinct, Repartition

Question #7Topic 1
The default value of spark.sql.shuffle.partitions is 200. Which of the following describes what that means?

A. By default, all DataFrames in Spark will be spit to perfectly fill the memory of 200 executors.
B. By default, new DataFrames created by Spark will be split to perfectly fill the memory of 200 executors.
C. By default, Spark will only read the first 200 partitions of DataFrames to improve speed.
D. By default, all DataFrames in Spark, including existing DataFrames, will be split into 200 unique segments for parallelization.
E. By default, DataFrames will be split into 200 unique partitions when data is being shuffled. Most Voted
 
Correct Answer: E 


Question #8Topic 1
Which of the following is the most complete description of lazy evaluation?

A. None of these options describe lazy evaluation
B. A process is lazily evaluated if its execution does not start until it is put into action by some type of trigger 
C. A process is lazily evaluated if its execution does not start until it is forced to display a result to the user
D. A process is lazily evaluated if its execution does not start until it reaches a specified date and time
E. A process is lazily evaluated if its execution does not start until it is finished compiling
 
Correct Answer: B 


Question #9Topic 1
Which of the following DataFrame operations is classified as an action?

A. DataFrame.drop()
B. DataFrame.coalesce()
C. DataFrame.take()
D. DataFrame.join()
E. DataFrame.filter()
 
Correct Answer: C 

Question #10Topic 1
Which of the following DataFrame operations is classified as a wide transformation?

A. DataFrame.filter()
B. DataFrame.join()
C. DataFrame.select()
D. DataFrame.drop()
E. DataFrame.union()
 
Correct Answer: B üó≥Ô∏è


Question #11Topic 1
Which of the following describes the difference between cluster and client execution modes?

A. The cluster execution mode runs the driver on a worker node within a cluster, while the client execution mode runs the driver on the client machine 
(also known as a gateway machine or edge node).
B. The cluster execution mode is run on a local cluster, while the client execution mode is run in the cloud.
C. The cluster execution mode distributes executors across worker nodes in a cluster, while the client execution mode runs a Spark job entirely on one client machine.
D. The cluster execution mode runs the driver on the cluster machine (also known as a gateway machine or edge node), while the client execution mode runs the driver on a worker node within a cluster.
E. The cluster execution mode distributes executors across worker nodes in a cluster, while the client execution mode submits a Spark job from a remote machine to be run on a remote, unconfigurable cluster.
 
Correct Answer: A 


Question #12Topic 1
Which of the following statements about Spark‚Äôs stability is incorrect?

A. Spark is designed to support the loss of any set of worker nodes.
B. Spark will rerun any failed tasks due to failed worker nodes.
C. Spark will recompute data cached on failed worker nodes.
D. Spark will spill data to disk if it does not fit in memory.
E. Spark will reassign the driver to a worker node if the driver‚Äôs node fails. Most Voted
 
Correct Answer: E üó≥Ô∏è


Question #13Topic 1
Which of the following cluster configurations is most likely to experience an out-of-memory error in response to data skew in a single partition?
image1
Note: each configuration has roughly the same compute power using 100 GB of RAM and 200 cores.

A. Scenario #4
B. Scenario #5
C. Scenario #6 Most Voted
D. More information is needed to determine an answer.
E. Scenario #1
 
Correct Answer: C üó≥Ô∏è
Community vote distribution
C (89%)


The most likely scenario to experience an out-of-memory error in response to data skew in a single partition is:

C. Scenario #6: 12.5 GB Worker Node, 12.5 GB Executor. 1 Driver & 8 Executors.

Explanation:

Data skew refers to an uneven distribution of data across partitions. When there is significant skew in a single partition,
it can lead to increased memory usage for that specific partition, potentially causing out-of-memory errors. 
The smaller the available memory per executor, the higher the likelihood of encountering such issues.

In this case, Scenario #6 has the smallest worker node and executor configuration, with only 12.5 GB of RAM available for each executor. With 8 executors,
the total available memory is still 100 GB (similar to other scenarios),
but the reduced memory per executor increases the risk of encountering out-of-memory errors when handling skewed data
in a single partition.



Question #14Topic 1
Of the following situations, in which will it be most advantageous to store DataFrame df at the MEMORY_AND_DISK 
storage level rather than the MEMORY_ONLY storage level?

A. When all of the computed data in DataFrame df can fit into memory.
B. When the memory is full and it‚Äôs faster to recompute all the data in DataFrame df rather than read it from disk.
C. When it‚Äôs faster to recompute all the data in DataFrame df that cannot fit into memory based on its logical plan rather than read it from disk.
D. When it‚Äôs faster to read all the computed data in DataFrame df that cannot fit into memory from disk rather than recompute it based on its logical plan. Most Voted
E. The storage level MENORY_ONLY will always be more advantageous because it‚Äôs faster to read data from memory than it is to read data from disk.
 
Correct Answer: D 


Question #15Topic 1
A Spark application has a 128 GB DataFrame A and a 1 GB DataFrame B. If a broadcast join were to be performed on these two DataFrames, which of the following describes which DataFrame should be broadcasted and why?

A. Either DataFrame can be broadcasted. Their results will be identical in result and efficiency.
B. DataFrame B should be broadcasted because it is smaller and will eliminate the need for the shuffling of itself.
C. DataFrame A should be broadcasted because it is larger and will eliminate the need for the shuffling of DataFrame B.
D. DataFrame B should be broadcasted because it is smaller and will eliminate the need for the shuffling of DataFrame A. 
E. DataFrame A should be broadcasted because it is smaller and will eliminate the need for the shuffling of itself.
 
Correct Answer: D üó≥Ô∏è



Question #16Topic 1
Which of the following operations can be used to create a new DataFrame that has 12 partitions from an original DataFrame df that has 8 partitions?

A. df.repartition(12) 
B. df.cache()
C. df.partitionBy(1.5)
D. df.coalesce(12)
E. df.partitionBy(12)
 
Correct Answer: A 



Question #17Topic 1
Which of the following object types cannot be contained within a column of a Spark DataFrame?

A. DataFrame 
B. String
C. Array
D. null
E. Vector
 
Correct Answer: A 


Question #18Topic 1
Which of the following operations can be used to create a DataFrame with a subset of columns from DataFrame storesDF that are specified by name?

A. storesDF.subset()
B. storesDF.select() 
C. storesDF.selectColumn()
D. storesDF.filter()
E. storesDF.drop()
 
Correct Answer: B üó≥Ô∏è


Question #19Topic 1
The code block shown below contains an error. The code block is intended to return a DataFrame containing all columns from DataFrame storesDF except for column sqft and column customerSatisfaction. Identify the error.
Code block:
storesDF.drop(sqft, customerSatisfaction)

A. The drop() operation only works if one column name is called at a time ‚Äì there should be two calls in succession like storesDF.drop("sqft").drop("customerSatisfaction").
B. The drop() operation only works if column names are wrapped inside the col() function like storesDF.drop(col(sqft), col(customerSatisfaction)).
C. There is no drop() operation for storesDF.
D. The sqft and customerSatisfaction column names should be quoted like "sqft" and "customerSatisfaction". 
E. The sqft and customerSatisfaction column names should be subset from the DataFrame storesDF like storesDF."sqft" and storesDF."customerSatisfaction".
 
Correct Answer: D


The error in the code block is that the column names sqft and customerSatisfaction should be quoted, like "sqft" and "customerSatisfaction", since they are strings.
The correct code block should be:
storesDF.drop("sqft", "customerSatisfaction")


Question #20Topic 1
Which of the following statements about Spark DataFrames is incorrect?

A. Spark DataFrames are the same as a data frame in Python or R.  Most Voted
B. Spark DataFrames are built on top of RDDs.
C. Spark DataFrames are immutable.
D. Spark DataFrames are distributed.
E. Spark DataFrames have common Structured APIs.
 
Correct Answer: A üó≥Ô∏è

Question #21Topic 1
Which of the following code blocks returns a DataFrame containing only the rows from DataFrame storesDF where the value in column sqft is less than or equal to 25,000 OR the value in column customerSatisfaction is greater than or equal to 30?

A. storesDF.filter(col("sqft") <= 25000 | col("customerSatisfaction") >= 30)
B. storesDF.filter(col("sqft") <= 25000 or col("customerSatisfaction") >= 30)
C. storesDF.filter(sqft <= 25000 or customerSatisfaction >= 30)
D. storesDF.filter(col(sqft) <= 25000 | col(customerSatisfaction) >= 30)
E. storesDF.filter((col("sqft") <= 25000) | (col("customerSatisfaction") >= 30)) Most Voted
 
Correct Answer: E

Question #22Topic 1
Which of the following code blocks returns a new DataFrame from DataFrame storesDF where column storeId is of the type string?

A. storesDF.withColumn("storeId, cast(col("storeId"), StringType()))
B. storesDF.withColumn("storeId, col("storeId").cast(StringType())) Most Voted
C. storesDF.withColumn("storeId, cast(storeId).as(StringType)
D. storesDF.withColumn("storeId, col(storeId).cast(StringType)
E. storesDF.withColumn("storeId, cast("storeId").as(StringType()))
 
Correct Answer: B 



Question #23Topic 1
Which of the following code blocks returns a new DataFrame with a new column employeesPerSqft that is the quotient of column numberOfEmployees and column sqft, both of which are from DataFrame storesDF? Note that column employeesPerSqft is not in the original DataFrame storesDF.

A. storesDF.withColumn("employeesPerSqft", col("numberOfEmployees") / col("sqft")) Most Voted
B. storesDF.withColumn("employeesPerSqft", "numberOfEmployees" / "sqft")
C. storesDF.select("employeesPerSqft", "numberOfEmployees" / "sqft")
D. storesDF.select("employeesPerSqft", col("numberOfEmployees") / col("sqft"))
E. storesDF.withColumn(col("employeesPerSqft"), col("numberOfEmployees") / col("sqft"))
 
Correct Answer: A 


Question #24Topic 1
The code block shown below should return a new DataFrame from DataFrame storesDF where column modality is the constant string "PHYSICAL", Assume DataFrame storesDF is the only defined language variable. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.
Code block:
storesDF. _1_(_2_,_3_(_4_))

A. 1. withColumn
2. "modality"
3. col
4. "PHYSICAL"
B. 1. withColumn
2. "modality"
3. lit
4. PHYSICAL
C. 1. withColumn
2. "modality"
3. lit
4. "PHYSICAL" Most Voted
D. 1. withColumn
2. "modality"
3. SrtringType
4. "PHYSICAL"
E. 1. newColumn
2. modality
3. SrtringType
4. PHYSICAL
 
Correct Answer: C 


Question #25Topic 1
Which of the following code blocks returns a DataFrame where column storeCategory from DataFrame storesDF is split at the underscore character into column storeValueCategory and column storeSizeCategory?
A sample of DataFrame storesDF is displayed below:
image2

A. (storesDF.withColumn("storeValueCategory", split(col("storeCategory"), "_")[1])
.withColumn("storeSizeCategory", split(col("storeCategory"), "_")[2]))
B. (storesDF.withColumn("storeValueCategory", col("storeCategory").split("_")[0])
.withColumn("storeSizeCategory", col("storeCategory").split("_")[1]))
C. (storesDF.withColumn("storeValueCategory", split(col("storeCategory"), "_")[0])
.withColumn("storeSizeCategory", split(col("storeCategory"), "_")[1])) Most Voted
D. (storesDF.withColumn("storeValueCategory", split("storeCategory", "_")[0])
.withColumn("storeSizeCategory", split("storeCategory", "_")[1]))
E. (storesDF.withColumn("storeValueCategory", col("storeCategory").split("_")[1])
.withColumn("storeSizeCategory", col("storeCategory").split("_")[2]))
 
Correct Answer: C üó≥Ô∏è


Question #26Topic 1
Which of the following code blocks returns a new DataFrame where column productCategories only has one word per row, resulting in a DataFrame with many more rows than DataFrame storesDF?
A sample of storesDF is displayed below:
image3

A. storesDF.withColumn("productCategories", explode(col("productCategories"))) Most Voted
B. storesDF.withColumn("productCategories", split(col("productCategories")))
C. storesDF.withColumn("productCategories", col("productCategories").explode())
D. storesDF.withColumn("productCategories", col("productCategories").split())
E. storesDF.withColumn("productCategories", explode("productCategories"))
 
Correct Answer: A üó≥Ô∏è
Community vote distribution



Question #27Topic 1
Which of the following code blocks returns a new DataFrame with column storeDescription where the pattern "Description: " has been removed from the beginning of column storeDescription in DataFrame storesDF?
A sample of DataFrame storesDF is below:
image4

A. storesDF.withColumn("storeDescription", regexp_replace(col("storeDescription"), "^Description: "))
B. storesDF.withColumn("storeDescription", col("storeDescription").regexp_replace("^Description: ", ""))
C. storesDF.withColumn("storeDescription", regexp_extract(col("storeDescription"), "^Description: ", ""))
D. storesDF.withColumn("storeDescription", regexp_replace("storeDescription", "^Description: ", ""))
E. storesDF.withColumn("storeDescription", regexp_replace(col("storeDescription"), "^Description: ", "")) Most Voted
 
Correct Answer: E 


Question #28Topic 1
Which of the following code blocks returns a new DataFrame where column division from DataFrame storesDF has been replaced and renamed to column state and column managerName from DataFrame storesDF has been replaced and renamed to column managerFullName?

A. (storesDF.withColumnRenamed(["division", "state"], ["managerName", "managerFullName"])
B. (storesDF.withColumn("state", col("division"))
.withColumn("managerFullName", col("managerName")))
C. (storesDF.withColumn("state", "division")
.withColumn("managerFullName", "managerName"))
D. (storesDF.withColumnRenamed("state", "division")
.withColumnRenamed("managerFullName", "managerName"))
E. (storesDF.withColumnRenamed("division", "state")
.withColumnRenamed("managerName", "managerFullName")) Most Voted
 
Correct Answer: E 



Question #29Topic 1
The code block shown contains an error. The code block is intended to return a new DataFrame where column sqft from DataFrame storesDF has had its missing values replaced with the value 30,000. Identify the error.
A sample of DataFrame storesDF is displayed below:
image5
Code block:
storesDF.na.fill(30000, col("sqft"))

A. The argument to the subset parameter of fill() should be a string column name or a list of string column names rather than a Column object. Most Voted
B. The na.fill() operation does not work and should be replaced by the dropna() operation.
C. he argument to the subset parameter of fill() should be a the numerical position of the column rather than a Column object.
D. The na.fill() operation does not work and should be replaced by the nafill() operation.
E. The na.fill() operation does not work and should be replaced by the fillna() operation.
 
Correct Answer: A üó≥Ô∏è



Question #30Topic 1
Which of the following operations fails to return a DataFrame with no duplicate rows?

A. DataFrame.dropDuplicates()
B. DataFrame.distinct()
C. DataFrame.drop_duplicates()
D. DataFrame.drop_duplicates(subset = None)
E. DataFrame.drop_duplicates(subset = "all") Most Voted
 
Correct Answer: E 

Option E is incorrect as "all" is not a valid value for the subset parameter in the drop_duplicates() method.
The correct value should be a column name or a list of column names to be used as the subset to identify duplicate rows.
All other options (A, B, C, and D) can be used to return a DataFrame with no duplicate rows. The dropDuplicates(), distinct(), and drop_duplicates() methods are
all equivalent and return a new DataFrame with distinct rows. The drop_duplicates() method also accepts a subset parameter to specify the columns to use for 
identifying duplicates, and when the subset parameter is not specified, all columns are used. 
Therefore, both option A and C are valid, and option D is also valid as it is equivalent to drop_duplicates() with no subset parameter.






===================================================================================

My Understanding --

narrow vs wide transformation --

when the data needs to be moved (exchanged) across partitions then it is wide 

narrow --
select, filter, drop, withcolumn, coalesce 

wide --
join, groupBy, agg, repartition 









