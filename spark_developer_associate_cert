# Exam Pattern --

Apache Spark Architecture and Components - 20%

Using Spark SQL - 20%

Developing Apache Spark‚Ñ¢ DataFrame/DataSet API Applications - 30%

Troubleshooting and Tuning Apache Spark DataFrame API Applications - 10%

Structured Streaming - 10%

Using Spark Connect to deploy applications - 5%

Using Pandas API on Apache Spark - 5%

# Exam syllabus --

Exam Outline
-------------------------------------------------------------------------------

Section 1: Apache Spark Architecture and Components
-------------------------------------------------------------------------------

‚óè Identify the advantages and challenges of implementing Spark.

‚óè Identify the role of core components of Apache Spark‚Ñ¢'s Architecture, including cluster,
driver node, worker nodes/executors, CPU cores, and memory.

‚óè Describe the architecture of Apache Spark‚Ñ¢, including DataFrame and Dataset concepts,
SparkSession lifecycle, caching, storage levels, and garbage collection.

‚óè Explain the Apache Spark‚Ñ¢ Architecture execution hierarchy..

‚óè Configure Spark partitioning in distributed data processing, including shuffles and partitions

‚óè Describe the execution patterns of the Apache Spark‚Ñ¢ engine, including actions,
transformations, and lazy evaluation.

‚óè Identify the features of the Apache Spark Modules, including Core, Spark SQL, DataFrames,
Pandas API on Spark, Structured Streaming, and MLib.


Section 2: Using Spark SQL
-------------------------------------------------------------------------------

‚óè Utilize common data sources such as JDBC, files, etc., to efficiently read from and write to
Spark DataFrames using Spark SQL, including overwriting and partitioning by column.

‚óè Execute SQL queries directly on files, including ORC Files, JSON Files, CSV Files, Text Files,
and Delta Files, and understand the different save modes for outputting data in Spark SQL.

‚óè Save data to persistent tables while applying sorting and partitioning to optimize data
retrieval.

‚óè Register DataFrames as temporary views in Spark SQL, allowing them to be queried with
SQL syntax.


Section 3: Developing Apache Spark‚Ñ¢ DataFrame/DataSet API Applications
-------------------------------------------------------------------------------

‚óè Manipulate columns, rows, and table structures by adding, dropping, splitting, renaming
column names, applying filters, and exploding arrays.

‚óè Perform data deduplication and validation operations on DataFrames.

‚óè Perform aggregate operations on DataFrames such as count, approximate count distinct,
and mean, summary.

‚óè Manipulate and utilize Date data type, such as Unix epoch to date string, and extract date
component.

‚óè Combine DataFrames with operations such as Inner join, left join, broadcast join, multiple
keys, cross join, union, and union all.

‚óè Manage input and output operations by writing, overwriting, and reading DataFrames with
schemas.

‚óè Perform operations on DataFrames such as sorting, iterating, printing schema, and
conversion between DataFrame and sequence/list formats.

‚óè Create and invoke user-defined functions with or without stateful operators, including
StateStores.

‚óè Describe different types of variables in Spark, including broadcast variables and
accumulators.

‚óè Describe the purpose and implementation of broadcast joins


Section 4: Troubleshooting and Tuning Apache Spark DataFrame API Applications.
-------------------------------------------------------------------------------

‚óè Implement performance tuning strategies & optimize cluster utilization, including
partitioning, repartitioning, coalescing, identifying data skew, and reducing shuffling

‚óè Describe Adaptive Query Execution (AQE) and its benefits.

‚óè Perform logging and monitoring of Spark applications - publish, customize, and analyze
Driver logs and Executor logs to diagnose out-of-memory errors, cluster underutilization,
etc.


Section 5: Structured Streaming
-------------------------------------------------------------------------------

‚óè Explain the Structured Streaming engine in Spark, including its functions, programming
model, micro-batch processing, exactly-once semantics, and fault tolerance mechanisms.

‚óè Create and write Streaming DataFrames and Streaming Datasets, including the basic output
modes and output sinks.

‚óè Perform basic operations on Streaming DataFrames and Streaming Datasets, such as
selection, projection, window and aggregation.

‚óè Perform Streaming Deduplication in Structured Streaming, both with and without watermark
usage.


Section 6: Using Spark Connect to deploy applications
-------------------------------------------------------------------------------

‚óè Describe the features of Spark Connect.

‚óè Describe the different deployment mode types (Client, Cluster, Local) in the Apache Spark‚Ñ¢
environment.


Section 7: Using Pandas API on Spark
-------------------------------------------------------------------------------

‚óè Explain the advantages of using Pandas API on Spark.

‚óè Create and invoke Pandas UDF


=================================================================================================================
=================================================================================================================

DATABRICKS SAMPLE QUESTIONS


Question 1

Objective - Utilize common data sources such as JDBC, files, etc. to efficiently read from
and write to Spark DataFrames using SparkSQL, including overwrite and partitioning by
column.

A data engineer needs to write a DataFrame df to a Parquet file, partitioned by the column country,
and overwrite any existing data at the destination path.
Which code should the data engineer use to accomplish this task in Apache Spark?
A. df.write.mode("append").partitionBy("country").parquet("/data/output")
B. df.write.partitionBy("country").parquet("/data/output")
C. df.write.mode("overwrite").parquet("/data/output")
D.
df.write.mode("overwrite").partitionBy("country").parquet("/data/output")


Ans - D

Question 2

Objective - Perform logging and monitoring of Spark applications - publish, customize, and
analyze Driver logs and Executor logs to diagnose out-of-memory errors, cluster
underutilization, etc.

An engineer notices a significant increase in the job execution time during the execution of a Spark
job. After some investigation, the engineer decides to check the logs produced by the Executors.
How should the engineer retrieve the Executor logs to diagnose performance issues in the Spark
application?

A. Use the command 'spark-submit' with the '--verbose' flag to print the logs to the console.
B. Locate the executor logs on the Spark master node, typically under the /tmp directory.
C. Use the Spark UI to select the stage and view the executor logs directly from the stages tab.
D. Fetch the logs by running a Spark job with the `spark-sql` CLI tool.

Ans - C

Question 3:

Objective: Manipulate columns, rows, and table structures by adding, dropping, splitting, renaming
column names, applying filters, and exploding arrays.

Which code block will replace the division column in the storesDF DataFrame with a new column
named state, and simultaneously replace and rename the mName column to managerName in the
resulting DataFrame?

A. storesDF = (storesDF.withColumn("state", col("division"))
.drop("division")
.withColumnRenamed("mName", "managerName"))
B. storesDF = (storesDF.withColumnRenamed("state", "division")
.withColumnRenamed("mName", "managerName"))
C. storesDF = (storesDF.withColumn(col("division"), state)
.withColumnRenamed("managerName", "mName"))
D. storesDF = (storesDF .drop("division")
.withColumnRenamed("state", lit("default_state"))
.withColumnRenamed(columns={"mName": "managerName"}))

Ans - A

Question 4:

Objective: Perform operations on DataFrames such as sorting, iterating, printing schema, and
conversion between DataFrame and sequence/list formats.

Given a DataFrame employeeDF with columns: name, department, salary, and age.
Which code snippet sorts a DataFrame by multiple columns in descending order, printing its
schema, and converting the DataFrame to a list of rows?

A. result = employeeDF.orderBy(desc("salary"), desc("age"))
employeeDF.printSchema()
row_list = result.toPandas().values.tolist()
B. result = employeeDF.sort("salary", "age", ascending=[False,
False]) employeeDF.schema()
row_list = result.toList()
C. result = (employeeDF .orderBy(col("salary").desc(),
col("age").desc()) .collect())
employeeDF.printSchema()
row_list = [row.asDict() for row in result]
D. result = (employeeDF
.sort(["salary", "age"], descending=True)
.show())
employeeDF.describe()
row_list = list(result)

Ans - C

Question 5:

Objective: Configure Spark partitioning in distributed data processing, including shuffles and
partitions.

What will be the impact of setting the default value of spark.sql.shuffle.partitions to 200?

A. DataFrames will be divided into 200 distinct partitions during data shuffling operations.
B. New DataFrames created by Spark will be partitioned to utilize the memory of 200 executors
optimally.
C. All DataFrames in Spark will be partitioned to occupy the memory of 200 executors perfectly.
D. Spark will only process the first 200 partitions of DataFrames to enhance performance.

Ans - A


Question 6:

Objective: Perform data deduplication and validation operations on DataFrames.

Which code fragment will return a new DataFrame from storesDF that excludes all rows
containing at least one missing value in any column?

A. storesDF.na.drop("all")
B. storesDF.na.drop(subset = "sqft")
C. storesDF.na.drop()
D. storesDF.dropna("all")

Ans - C


Question 7:

Objective: Describe the different deployment mode types (Client, Cluster, Local) in Apache Spark
environment.

Which Spark deployment mode requires all executors to run on a single worker node?

A. Cluster mode
B. Local mode
C. Client mode
D. Standard mode

Ans - B 

Question 8:

Objective: Explain the Structured Streaming engine in Spark, including its functions, programming
model, micro-batch processing, exactly-once semantics, and fault tolerance mechanisms.

A developer must calculate real-time, rolling metrics like "average session duration in the last hour"
and "top 10 products viewed in the last 15 minutes" from continuous clickstream data for
recommendation engines. These metrics must update every 2 minutes. They chose Streaming
DataFrames over traditional batch DataFrames.

Why should the developer consider Streaming DataFrames in this use case?

A. Streaming DataFrames automatically handle data partitioning and caching more efficiently than
batch DataFrames, making them faster for processing large volumes of clickstream data, even
when processing historical data.
B. Streaming DataFrames enable continuous data processing with incremental updates to
aggregations, allowing real-time metrics without reprocessing the entire dataset every 2 minutes.
C. Streaming DataFrames provide better error handling and automatic retry mechanisms than
batch DataFrames, which is essential for handling unreliable network connections from the network
recovery mechanism.
D. Streaming DataFrames use more memory than batch DataFrames because they process data in
batches, making them ideal for this scenario.

Ans - B

Question 9:

Objective: Create and invoke user-defined functions with or without stateful operators, including
StateStores.

A developer is building a streaming application. The application's validation logic involves
computations and string manipulations unavailable in built-in Spark functions. The developer
wants to ensure the streaming job is stateless for each transaction and can scale horizontally
without maintaining customer history.

When should the developer use user-defined functions (UDFs) without stateful operators for this
transaction validation pipeline?

A. Maintain running totals and compare them to historical averages in state.
B. Perform windowed aggregations over the last 24 hours with incremental updates.
C. Detect session-based patterns by tracking sequences across events.
D. Apply custom logic without relying on previous transaction data or state between transaction

Ans - D


Question 10:

Objective: Perform aggregate operations on DataFrames such as count, approximate count
distinct, and mean, summary.

A developer needs a dashboard for a mobile app's 50 million user activity records, showing total
event and unique user counts by time. The dashboard refreshes hourly, and management
prioritizes speed and accepts a 2-3% error margin for unique user counts.

Why should the developer choose approx_count_distinct() over count(distinct())?

A. approx_count_distinct() offers improved accuracy over count(distinct()) for large
datasets due to its advanced statistical algorithms.
B. approx_count_distinct() handles nulls and missing data better than
count(distinct()), which is crucial for incomplete user activity logs.
C. approx_count_distinct() uses probabilistic algorithms (HyperLogLog) to boost
performance by avoiding costly shuffle operations.
D. approx_count_distinct() uses less memory per partition than count(distinct())
because it stores aggregated results in compressed format,

Ans - C 



---------------------------------------------------------------------------------------
UDEMY TEST - 1 :

Question 1 --

A data engineer needs to write a DataFrame df to a Parquet file, partitioned by the column country, and overwrite any existing data at the destination path.

Which code should the data engineer use to accomplish this task in Apache Spark?

Adf.write.mode('overwrite').partitionBy('country').parquet('/data/output')
Bdf.write.mode('append').partitionBy('country').parquet('/data/output')
Cdf.write.mode('overwrite').parquet('/data/output')
Ddf.write.partitionBy('country').parquet('/data/output')


Answer : A

The .mode('overwrite') ensures that existing files at the path will be replaced.

.partitionBy('country') optimizes queries by writing data into partitioned folders.

Correct syntax:

df.write.mode('overwrite').partitionBy('country').parquet('/data/output')

--- Source: Spark SQL, DataFrames and Datasets Guide



Question 2 --

A data engineer is reviewing a Spark application that applies several transformations to a DataFrame but notices that the job does not start executing immediately.

Which two characteristics of Apache Spark's execution model explain this behavior?

Choose 2 answers:

A. The Spark engine requires manual intervention to start executing transformations.
B. Only actions trigger the execution of the transformation pipeline.
C. Transformations are executed immediately to build the lineage graph.
D. The Spark engine optimizes the execution plan during the transformations, causing delays.
E. Transformations are evaluated lazily.

Ans - B,E


Question 3
Which command overwrites an existing JSON file when writing a DataFrame?

A. df.write.mode('overwrite').json('path/to/file')
B. df.write.overwrite.json('path/to/file')
C. df.write.json('path/to/file', overwrite=True)
D. df.write.format('json').save('path/to/file', mode='overwrite')
Ans -- A 

Question 4
What is the benefit of using Pandas on Spark for data transformations?

Options:

A. It is available only with Python, thereby reducing the learning curve.
B. It computes results immediately using eager execution, making it simple to use.
C. It runs on a single node only, utilizing the memory with memory-bound DataFrames and hence cost-efficient.
D. It executes queries faster using all the available cores in the cluster as well as provides Pandas's rich set of features.


Answer : D

Pandas API on Spark (formerly Koalas) offers:

Familiar Pandas-like syntax

Distributed execution using Spark under the hood

Scalability for large datasets across the cluster

It provides the power of Spark while retaining the productivity of Pandas.


Question 5
A data scientist is working with a Spark DataFrame called customerDF that contains customer information. The DataFrame has a column named email with customer email addresses. The data scientist needs to split this column into username and domain parts.

Which code snippet splits the email column into username and domain columns?

A.

customerDF.select(

col("email").substr(0, 5).alias("username"),

col("email").substr(-5).alias("domain")

)

B.

customerDF.withColumn("username", split(col("email"), "@").getItem(0)) \

.withColumn("domain", split(col("email"), "@").getItem(1))

C.

customerDF.withColumn("username", substring_index(col("email"), "@", 1)) \

.withColumn("domain", substring_index(col("email"), "@", -1))

D.

customerDF.select(

regexp_replace(col("email"), "@", "").alias("username"),

regexp_replace(col("email"), "@", "").alias("domain")

)

AOption A
BOption B
COption C
DOption D


Answer : B

Option B is the correct and idiomatic approach in PySpark to split a string column (like email) based on a delimiter such as '@'.

The split(col('email'), '@') function returns an array with two elements: username and domain.

getItem(0) retrieves the first part (username).

getItem(1) retrieves the second part (domain).

withColumn() is used to create new columns from the extracted values.

Example from official Databricks Spark documentation on splitting columns:

from pyspark.sql.functions import split, col

df.withColumn('username', split(col('email'), '@').getItem(0)) \

.withColumn('domain', split(col('email'), '@').getItem(1))

Why other options are incorrect:

A uses fixed substring indices (substr(0, 5)), which won't correctly extract usernames and domains of varying lengths.

C uses substring_index, which is available but less idiomatic for splitting emails and is slightly less readable.

D removes '@' from the email entirely, losing the separation between username and domain, and ends up duplicating values in both fields.

Therefore, Option B is the most accurate and reliable solution according to Apache Spark 3.5 best practices.



Question 6
Which feature of Spark Connect is considered when designing an application to enable remote interaction with the Spark cluster?

A. It provides a way to run Spark applications remotely in any programming language
B. It can be used to interact with any remote cluster using the REST API
C. It allows for remote execution of Spark jobs
D. It is primarily used for data ingestion into Spark from external sources


Answer : C

Spark Connect introduces a decoupled client-server architecture. Its key feature is enabling Spark job submission and execution from remote clients --- in Python, Java, etc.

From Databricks documentation:

''Spark Connect allows remote clients to connect to a Spark cluster and execute Spark jobs without being co-located with the Spark driver.''

A is close, but 'any language' is overstated (currently supports Python, Java, etc., not literally all).

B refers to REST, which is not Spark Connect's mechanism.

D is incorrect; Spark Connect isn't focused on ingestion.

Final Answer: C



Question 7
A developer runs:

df.partitionBy("country","colour")

What is the result?

Options:

A. It stores all data in a single Parquet file.
B. It throws an error if there are null values in either partition column.
C. It appends new partitions to an existing Parquet file.
D. It creates separate directories for each unique combination of color and fruit.


Answer : D

The partitionBy() method in Spark organizes output into subdirectories based on unique combinations of the specified columns:

e.g.

/path/to/output/color=red/fruit=apple/part-0000.parquet

/path/to/output/color=green/fruit=banana/part-0001.parquet

This improves query performance via partition pruning.

It does not consolidate into a single file.

Null values are allowed in partitions.

It does not 'append' unless .mode('append') is used.



--------------------------------------------------------------------
EXAMTOPICS Ques --

Question #1Topic 1
Which of the following describes the Spark driver?

A. The Spark driver is responsible for performing all execution in all execution modes ‚Äì it is the entire Spark application.
B. The Spare driver is fault tolerant ‚Äì if it fails, it will recover the entire Spark application.
C. The Spark driver is the coarsest level of the Spark execution hierarchy ‚Äì it is synonymous with the Spark application.
D. The Spark driver is the program space in which the Spark application‚Äôs main method runs coordinating the Spark entire application. Most Voted
E. The Spark driver is horizontally scaled to increase overall processing throughput of a Spark application.
 
Correct Answer: D 


Question #2Topic 1
Which of the following describes the relationship between nodes and executors?

A. Executors and nodes are not related.
B. Anode is a processing engine running on an executor.
C. An executor is a processing engine running on a node. Most Voted
D. There are always the same number of executors and nodes.
E. There are always more nodes than executors.
 
Correct Answer: C üó≥Ô∏è


Question #3Topic 1
Which of the following will occur if there are more slots than there are tasks?

A. The Spark job will likely not run as efficiently as possible. Most Voted
B. The Spark application will fail ‚Äì there must be at least as many tasks as there are slots.
C. Some executors will shut down and allocate all slots on larger executors first.
D. More tasks will be automatically generated to ensure all slots are being used.
E. The Spark job will use just one single slot to perform all tasks.
 
Correct Answer: A üó≥Ô∏è
Community vote distribution


Question #4Topic 1
Which of the following is the most granular level of the Spark execution hierarchy?

A. Task       
B. Executor
C. Node
D. Job
E. Slot
 
Correct Answer: A 


Question #5Topic 1
Which of the following statements about Spark jobs is incorrect?

A. Jobs are broken down into stages.
B. There are multiple tasks within a single job when a DataFrame has more than one partition.
C. Jobs are collections of tasks that are divided up based on when an action is called.
D. There is no way to monitor the progress of a job. Most Voted
E. Jobs are collections of tasks that are divided based on when language variables are defined.
 
Correct Answer: D üó≥Ô∏è
Community vote distribution
D (100%)


Question #6Topic 1
Which of the following operations is most likely to result in a shuffle?

A. DataFrame.join() 
B. DataFrame.filter()
C. DataFrame.union()
D. DataFrame.where()
E. DataFrame.drop()
 
Correct Answer: A 
-- Join, GroupBy, Distinct, Repartition

Question #7Topic 1
The default value of spark.sql.shuffle.partitions is 200. Which of the following describes what that means?

A. By default, all DataFrames in Spark will be spit to perfectly fill the memory of 200 executors.
B. By default, new DataFrames created by Spark will be split to perfectly fill the memory of 200 executors.
C. By default, Spark will only read the first 200 partitions of DataFrames to improve speed.
D. By default, all DataFrames in Spark, including existing DataFrames, will be split into 200 unique segments for parallelization.
E. By default, DataFrames will be split into 200 unique partitions when data is being shuffled. Most Voted
 
Correct Answer: E 


Question #8Topic 1
Which of the following is the most complete description of lazy evaluation?

A. None of these options describe lazy evaluation
B. A process is lazily evaluated if its execution does not start until it is put into action by some type of trigger 
C. A process is lazily evaluated if its execution does not start until it is forced to display a result to the user
D. A process is lazily evaluated if its execution does not start until it reaches a specified date and time
E. A process is lazily evaluated if its execution does not start until it is finished compiling
 
Correct Answer: B 


Question #9Topic 1
Which of the following DataFrame operations is classified as an action?

A. DataFrame.drop()
B. DataFrame.coalesce()
C. DataFrame.take()
D. DataFrame.join()
E. DataFrame.filter()
 
Correct Answer: C 

Question #10Topic 1
Which of the following DataFrame operations is classified as a wide transformation?

A. DataFrame.filter()
B. DataFrame.join()
C. DataFrame.select()
D. DataFrame.drop()
E. DataFrame.union()
 
Correct Answer: B üó≥Ô∏è


Question #11Topic 1
Which of the following describes the difference between cluster and client execution modes?

A. The cluster execution mode runs the driver on a worker node within a cluster, while the client execution mode runs the driver on the client machine 
(also known as a gateway machine or edge node).
B. The cluster execution mode is run on a local cluster, while the client execution mode is run in the cloud.
C. The cluster execution mode distributes executors across worker nodes in a cluster, while the client execution mode runs a Spark job entirely on one client machine.
D. The cluster execution mode runs the driver on the cluster machine (also known as a gateway machine or edge node), while the client execution mode runs the driver on a worker node within a cluster.
E. The cluster execution mode distributes executors across worker nodes in a cluster, while the client execution mode submits a Spark job from a remote machine to be run on a remote, unconfigurable cluster.
 
Correct Answer: A 


Question #12Topic 1
Which of the following statements about Spark‚Äôs stability is incorrect?

A. Spark is designed to support the loss of any set of worker nodes.
B. Spark will rerun any failed tasks due to failed worker nodes.
C. Spark will recompute data cached on failed worker nodes.
D. Spark will spill data to disk if it does not fit in memory.
E. Spark will reassign the driver to a worker node if the driver‚Äôs node fails. Most Voted
 
Correct Answer: E üó≥Ô∏è


Question #13Topic 1
Which of the following cluster configurations is most likely to experience an out-of-memory error in response to data skew in a single partition?
image1
Note: each configuration has roughly the same compute power using 100 GB of RAM and 200 cores.

A. Scenario #4
B. Scenario #5
C. Scenario #6 Most Voted
D. More information is needed to determine an answer.
E. Scenario #1
 
Correct Answer: C üó≥Ô∏è
Community vote distribution
C (89%)


The most likely scenario to experience an out-of-memory error in response to data skew in a single partition is:

C. Scenario #6: 12.5 GB Worker Node, 12.5 GB Executor. 1 Driver & 8 Executors.

Explanation:

Data skew refers to an uneven distribution of data across partitions. When there is significant skew in a single partition,
it can lead to increased memory usage for that specific partition, potentially causing out-of-memory errors. 
The smaller the available memory per executor, the higher the likelihood of encountering such issues.

In this case, Scenario #6 has the smallest worker node and executor configuration, with only 12.5 GB of RAM available for each executor. With 8 executors,
the total available memory is still 100 GB (similar to other scenarios),
but the reduced memory per executor increases the risk of encountering out-of-memory errors when handling skewed data
in a single partition.



Question #14Topic 1
Of the following situations, in which will it be most advantageous to store DataFrame df at the MEMORY_AND_DISK 
storage level rather than the MEMORY_ONLY storage level?

A. When all of the computed data in DataFrame df can fit into memory.
B. When the memory is full and it‚Äôs faster to recompute all the data in DataFrame df rather than read it from disk.
C. When it‚Äôs faster to recompute all the data in DataFrame df that cannot fit into memory based on its logical plan rather than read it from disk.
D. When it‚Äôs faster to read all the computed data in DataFrame df that cannot fit into memory from disk rather than recompute it based on its logical plan. Most Voted
E. The storage level MENORY_ONLY will always be more advantageous because it‚Äôs faster to read data from memory than it is to read data from disk.
 
Correct Answer: D 


Question #15Topic 1
A Spark application has a 128 GB DataFrame A and a 1 GB DataFrame B. If a broadcast join were to be performed on these two DataFrames, which of the following describes which DataFrame should be broadcasted and why?

A. Either DataFrame can be broadcasted. Their results will be identical in result and efficiency.
B. DataFrame B should be broadcasted because it is smaller and will eliminate the need for the shuffling of itself.
C. DataFrame A should be broadcasted because it is larger and will eliminate the need for the shuffling of DataFrame B.
D. DataFrame B should be broadcasted because it is smaller and will eliminate the need for the shuffling of DataFrame A. 
E. DataFrame A should be broadcasted because it is smaller and will eliminate the need for the shuffling of itself.
 
Correct Answer: D üó≥Ô∏è



Question #16Topic 1
Which of the following operations can be used to create a new DataFrame that has 12 partitions from an original DataFrame df that has 8 partitions?

A. df.repartition(12) 
B. df.cache()
C. df.partitionBy(1.5)
D. df.coalesce(12)
E. df.partitionBy(12)
 
Correct Answer: A 



Question #17Topic 1
Which of the following object types cannot be contained within a column of a Spark DataFrame?

A. DataFrame 
B. String
C. Array
D. null
E. Vector
 
Correct Answer: A 


Question #18Topic 1
Which of the following operations can be used to create a DataFrame with a subset of columns from DataFrame storesDF that are specified by name?

A. storesDF.subset()
B. storesDF.select() 
C. storesDF.selectColumn()
D. storesDF.filter()
E. storesDF.drop()
 
Correct Answer: B üó≥Ô∏è


Question #19Topic 1
The code block shown below contains an error. The code block is intended to return a DataFrame containing all columns from DataFrame storesDF except for column sqft and column customerSatisfaction. Identify the error.
Code block:
storesDF.drop(sqft, customerSatisfaction)

A. The drop() operation only works if one column name is called at a time ‚Äì there should be two calls in succession like storesDF.drop("sqft").drop("customerSatisfaction").
B. The drop() operation only works if column names are wrapped inside the col() function like storesDF.drop(col(sqft), col(customerSatisfaction)).
C. There is no drop() operation for storesDF.
D. The sqft and customerSatisfaction column names should be quoted like "sqft" and "customerSatisfaction". 
E. The sqft and customerSatisfaction column names should be subset from the DataFrame storesDF like storesDF."sqft" and storesDF."customerSatisfaction".
 
Correct Answer: D


The error in the code block is that the column names sqft and customerSatisfaction should be quoted, like "sqft" and "customerSatisfaction", since they are strings.
The correct code block should be:
storesDF.drop("sqft", "customerSatisfaction")


Question #20Topic 1
Which of the following statements about Spark DataFrames is incorrect?

A. Spark DataFrames are the same as a data frame in Python or R.  Most Voted
B. Spark DataFrames are built on top of RDDs.
C. Spark DataFrames are immutable.
D. Spark DataFrames are distributed.
E. Spark DataFrames have common Structured APIs.
 
Correct Answer: A üó≥Ô∏è

Question #21Topic 1
Which of the following code blocks returns a DataFrame containing only the rows from DataFrame storesDF where the value in column sqft is less than or equal to 25,000 OR the value in column customerSatisfaction is greater than or equal to 30?

A. storesDF.filter(col("sqft") <= 25000 | col("customerSatisfaction") >= 30)
B. storesDF.filter(col("sqft") <= 25000 or col("customerSatisfaction") >= 30)
C. storesDF.filter(sqft <= 25000 or customerSatisfaction >= 30)
D. storesDF.filter(col(sqft) <= 25000 | col(customerSatisfaction) >= 30)
E. storesDF.filter((col("sqft") <= 25000) | (col("customerSatisfaction") >= 30)) Most Voted
 
Correct Answer: E

Question #22Topic 1
Which of the following code blocks returns a new DataFrame from DataFrame storesDF where column storeId is of the type string?

A. storesDF.withColumn("storeId, cast(col("storeId"), StringType()))
B. storesDF.withColumn("storeId, col("storeId").cast(StringType())) Most Voted
C. storesDF.withColumn("storeId, cast(storeId).as(StringType)
D. storesDF.withColumn("storeId, col(storeId).cast(StringType)
E. storesDF.withColumn("storeId, cast("storeId").as(StringType()))
 
Correct Answer: B 



Question #23Topic 1
Which of the following code blocks returns a new DataFrame with a new column employeesPerSqft that is the quotient of column numberOfEmployees and column sqft, both of which are from DataFrame storesDF? Note that column employeesPerSqft is not in the original DataFrame storesDF.

A. storesDF.withColumn("employeesPerSqft", col("numberOfEmployees") / col("sqft")) Most Voted
B. storesDF.withColumn("employeesPerSqft", "numberOfEmployees" / "sqft")
C. storesDF.select("employeesPerSqft", "numberOfEmployees" / "sqft")
D. storesDF.select("employeesPerSqft", col("numberOfEmployees") / col("sqft"))
E. storesDF.withColumn(col("employeesPerSqft"), col("numberOfEmployees") / col("sqft"))
 
Correct Answer: A 


Question #24Topic 1
The code block shown below should return a new DataFrame from DataFrame storesDF where column modality is the constant string "PHYSICAL", Assume DataFrame storesDF is the only defined language variable. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.
Code block:
storesDF. _1_(_2_,_3_(_4_))

A. 1. withColumn
2. "modality"
3. col
4. "PHYSICAL"
B. 1. withColumn
2. "modality"
3. lit
4. PHYSICAL
C. 1. withColumn
2. "modality"
3. lit
4. "PHYSICAL" Most Voted
D. 1. withColumn
2. "modality"
3. SrtringType
4. "PHYSICAL"
E. 1. newColumn
2. modality
3. SrtringType
4. PHYSICAL
 
Correct Answer: C 


Question #25Topic 1
Which of the following code blocks returns a DataFrame where column storeCategory from DataFrame storesDF is split at the underscore character into column storeValueCategory and column storeSizeCategory?
A sample of DataFrame storesDF is displayed below:
image2

A. (storesDF.withColumn("storeValueCategory", split(col("storeCategory"), "_")[1])
.withColumn("storeSizeCategory", split(col("storeCategory"), "_")[2]))
B. (storesDF.withColumn("storeValueCategory", col("storeCategory").split("_")[0])
.withColumn("storeSizeCategory", col("storeCategory").split("_")[1]))
C. (storesDF.withColumn("storeValueCategory", split(col("storeCategory"), "_")[0])
.withColumn("storeSizeCategory", split(col("storeCategory"), "_")[1])) Most Voted
D. (storesDF.withColumn("storeValueCategory", split("storeCategory", "_")[0])
.withColumn("storeSizeCategory", split("storeCategory", "_")[1]))
E. (storesDF.withColumn("storeValueCategory", col("storeCategory").split("_")[1])
.withColumn("storeSizeCategory", col("storeCategory").split("_")[2]))
 
Correct Answer: C üó≥Ô∏è


Question #26Topic 1
Which of the following code blocks returns a new DataFrame where column productCategories only has one word per row, resulting in a DataFrame with many more rows than DataFrame storesDF?
A sample of storesDF is displayed below:
image3

A. storesDF.withColumn("productCategories", explode(col("productCategories"))) Most Voted
B. storesDF.withColumn("productCategories", split(col("productCategories")))
C. storesDF.withColumn("productCategories", col("productCategories").explode())
D. storesDF.withColumn("productCategories", col("productCategories").split())
E. storesDF.withColumn("productCategories", explode("productCategories"))
 
Correct Answer: A üó≥Ô∏è
Community vote distribution



Question #27Topic 1
Which of the following code blocks returns a new DataFrame with column storeDescription where the pattern "Description: " has been removed from the beginning of column storeDescription in DataFrame storesDF?
A sample of DataFrame storesDF is below:
image4

A. storesDF.withColumn("storeDescription", regexp_replace(col("storeDescription"), "^Description: "))
B. storesDF.withColumn("storeDescription", col("storeDescription").regexp_replace("^Description: ", ""))
C. storesDF.withColumn("storeDescription", regexp_extract(col("storeDescription"), "^Description: ", ""))
D. storesDF.withColumn("storeDescription", regexp_replace("storeDescription", "^Description: ", ""))
E. storesDF.withColumn("storeDescription", regexp_replace(col("storeDescription"), "^Description: ", "")) Most Voted
 
Correct Answer: E 


Question #28Topic 1
Which of the following code blocks returns a new DataFrame where column division from DataFrame storesDF has been replaced and renamed to column state and column managerName from DataFrame storesDF has been replaced and renamed to column managerFullName?

A. (storesDF.withColumnRenamed(["division", "state"], ["managerName", "managerFullName"])
B. (storesDF.withColumn("state", col("division"))
.withColumn("managerFullName", col("managerName")))
C. (storesDF.withColumn("state", "division")
.withColumn("managerFullName", "managerName"))
D. (storesDF.withColumnRenamed("state", "division")
.withColumnRenamed("managerFullName", "managerName"))
E. (storesDF.withColumnRenamed("division", "state")
.withColumnRenamed("managerName", "managerFullName")) Most Voted
 
Correct Answer: E 



Question #29Topic 1
The code block shown contains an error. The code block is intended to return a new DataFrame where column sqft from DataFrame storesDF has had its missing values replaced with the value 30,000. Identify the error.
A sample of DataFrame storesDF is displayed below:
image5
Code block:
storesDF.na.fill(30000, col("sqft"))

A. The argument to the subset parameter of fill() should be a string column name or a list of string column names rather than a Column object. Most Voted
B. The na.fill() operation does not work and should be replaced by the dropna() operation.
C. he argument to the subset parameter of fill() should be a the numerical position of the column rather than a Column object.
D. The na.fill() operation does not work and should be replaced by the nafill() operation.
E. The na.fill() operation does not work and should be replaced by the fillna() operation.
 
Correct Answer: A üó≥Ô∏è



Question #30Topic 1
Which of the following operations fails to return a DataFrame with no duplicate rows?

A. DataFrame.dropDuplicates()
B. DataFrame.distinct()
C. DataFrame.drop_duplicates()
D. DataFrame.drop_duplicates(subset = None)
E. DataFrame.drop_duplicates(subset = "all") Most Voted
 
Correct Answer: E 

Option E is incorrect as "all" is not a valid value for the subset parameter in the drop_duplicates() method.
The correct value should be a column name or a list of column names to be used as the subset to identify duplicate rows.
All other options (A, B, C, and D) can be used to return a DataFrame with no duplicate rows. The dropDuplicates(), distinct(), and drop_duplicates() methods are
all equivalent and return a new DataFrame with distinct rows. The drop_duplicates() method also accepts a subset parameter to specify the columns to use for 
identifying duplicates, and when the subset parameter is not specified, all columns are used. 
Therefore, both option A and C are valid, and option D is also valid as it is equivalent to drop_duplicates() with no subset parameter.


----------------------------------------


Which of the following DataFrame operations are wide transformations (that is, they result in a shuffle)?
*A. repartition()
B. filter()
*C. orderBy()
*D. distinct()
E. drop()
F. cache()



Which of the following methods are NOT a DataFrame action?
*A. limit()
B. foreach()
C. first()
*D. printSchema()
E. show()
*F. cache()



Which of the following statements about Spark accumulator variables is NOT true?
A. For accumulator updates performed inside actions only, Spark guarantees that each task‚Äôs update to the accumulator will be applied only once, meaning that restarted tasks will not update the value. In transformations, each task‚Äôs update can be applied more than once if tasks or job stages are re-executed.
B. Accumulators provide a shared, mutable variable that a Spark cluster can safely update on a per-row basis.
C. You can define your own custom accumulator class by extending org.apache.spark.util.AccumulatorV2 in Java or Scala or pyspark.AccumulatorParam in Python.
*D. The Spark UI displays all accumulators used by your application.

Ref: D is FALSE, as spark ui only displays named accumulators.


Ques -
import org.apache.spark.sql.functions._

val a = Array(1002, 3001, 4002, 2003, 2002, 3004, 1003, 4006)

val b = spark
  .createDataset(a)
  .withColumn("x", col("value") % 1000)

val c = b
  .groupBy(col("x"))
  .agg(count("x"), sum("value"))
  .drop("x")
  .toDF("count", "total")
  .orderBy(col("count").desc, col("total"))
  .limit(1)
  .show()


A.		
+-----+-----+
|count|total|
+-----+-----+
|    3| 7006|
+-----+-----+
 
B.		
+-----+-----+
|count|total|
+-----+-----+
|    1| 3001|
+-----+-----+
 
C.		
+-----+-----+
|count|total|
+-----+-----+
|    2| 8008|
+-----+-----+
D.		
+-----+-----+
|count|total|
+-----+-----+
|    8|20023|
+-----+-----+

Ans - A


Ques - Given an instance of SparkSession named spark, which one of the following code fragments executemost quickly and produce a DataFrame with the specified schema? Assume a variable named schema with the correctly structured StructType to represent the DataFrame's schema has already been initialized.
Sample data:

id,firstName,lastName,birthDate,email,country,phoneNumber 1,Pennie,Hirschmann,2017-12-03,ph123@databricks.com,US,+1(123)4567890

Schema:

id: integer
firstName: string
lastName: string
birthDate: timestamp
email: string
county: string
phoneNumber: string
 	
A.		
val df = spark.read
   .option("inferSchema", "true")
   .option("header", "true")
   .csv("/data/people.csv")
B.		
val df = spark.read
   .option("inferSchema", "true")
   .schema(schema)
   .csv("/data/people.csv")
C.		
val df = spark.read
   .schema(schema)
   .option("sep", ",")
   .csv("/data/people.csv")
D.		
val df = spark.read
   .schema(schema)
   .option("header", "true")
   .csv("/data/people.csv")

ANs - D


Ques --

val rawData = Seq(
  (1, 1000, "Apple", 0.76),
  (2, 1000, "Apple", 0.11),
  (1, 2000, "Orange", 0.98),
  (1, 3000, "Banana", 0.24),
  (2, 3000, "Banana", 0.99)
)
val dfA = spark.createDataFrame(rawData).toDF("UserKey", "ItemKey", "ItemName", "Score")



Ques --
Select the code fragment that produces the following result:
 

+-------+-----------------------------------------------------------------+
|UserKey|Collection                                                       |
+-------+-----------------------------------------------------------------+
|1      |[[0.98, 2000, Orange], [0.76, 1000, Apple], [0.24, 3000, Banana]]|
|2      |[[0.99, 3000, Banana], [0.11, 1000, Apple]]                      |
+-------+-----------------------------------------------------------------+
 	
A.		
import org.apache.spark.sql.expressions.Window
dfA.withColumn(
    "Collection",
    collect_list(struct("Score", "ItemKey", "ItemName")).over(Window.partitionBy("ItemKey"))
  )
  .select("UserKey", "Collection")
  .show(20, false)
 
B.		
dfA.groupBy("UserKey")
  .agg(collect_list(struct("Score", "ItemKey", "ItemName")))
  .toDF("UserKey", "Collection")
  .show(20, false)
 
C.		
dfA.groupBy("UserKey", "ItemKey", "ItemName")
  .agg(sort_array(collect_list(struct("Score", "ItemKey", "ItemName")), false))
  .drop("ItemKey", "ItemName")
  .toDF("UserKey", "Collection")
  .show(20, false)
 
D.		
dfA.groupBy("UserKey")
  .agg(sort_array(collect_list(struct("Score", "ItemKey", "ItemName")), false))
  .toDF("UserKey", "Collection")
  .show(20, false)

Ans - D

Ques --
tableA is a DataFrame consisting of 20 fields and 40 billion rows of data with a surrogate key field. tableB is a DataFrame functioning as a lookup table for the surrogate key consisting of 2 fields and 5,000 rows. If the in-memory size of tableB is 22MB, what occurs when the following code is executed:
val df = tableA.join(broadcast(tableB), Seq("primary_key"))
A. The broadcast function is non-deterministic, thus a BroadcastHashJoin is likely to occur, but isn't guaranteed to occur.
B. A normal hash join will be executed with a shuffle phase since the broadcast table is greater than the 10MB default threshold and the broadcast command can be overridden silently by the Catalyst optimizer.
C. The contents of tableB will be replicated and sent to each executor to eliminate the need for a shuffle stage during the join.
D. An exception will be thrown due to tableB being greater than the 10MB default threshold for a broadcast join.


Ans - B


Ques --
Consider the following DataFrame:
import org.apache.spark.sql.functions._

val people = Seq(
    ("Ali", 0, Seq(100)),
    ("Barbara", 1, Seq(300, 250, 100)),
    ("Cesar", 1, Seq(350, 100)),
    ("Dongmei", 1, Seq(400, 100)),
    ("Eli", 2, Seq(250)),
    ("Florita", 2, Seq(500, 300, 100)),
    ("Gatimu", 3, Seq(300, 100))
  )
  .toDF("name", "department", "score")



+----------+-------+-------+
|department|   name|highest|
+----------+-------+-------+
|         0|    Ali|    100|
|         1|Dongmei|    400|
|         2|Florita|    500|
|         3| Gatimu|    300|
+----------+-------+-------+
 	
A.		
val maxByDept = people
  .withColumn("score", explode(col("score")))
  .groupBy("department")
  .max("score")
  .withColumnRenamed("max(score)", "highest")

maxByDept
  .join(people, "department")
  .select("department", "name", "highest")
  .orderBy("department")
  .dropDuplicates("department")
  .show()
 
B.		
people
  .withColumn("score", explode(col("score")))
  .orderBy("department", "score")
  .select(col("name"), col("department"), first(col("score")).as("highest"))
  .show()
 
C.		
import org.apache.spark.sql.expressions.Window

val windowSpec = Window.partitionBy("department").orderBy(col("score").desc)

people
  .withColumn("score", explode(col("score")))
  .select(
    col("department"),
    col("name"),
    dense_rank().over(windowSpec).alias("rank"),
    max(col("score")).over(windowSpec).alias("highest")
  )
  .where(col("rank") === 1)
  .drop("rank")
  .orderBy("department")
  .show()
D.		
people
  .withColumn("score", explode(col("score")))
  .groupBy("department")
  .max("score")
  .withColumnRenamed("max(score)", "highest")
  .orderBy("department")
  .show()

Ans -- C 


Ques --
Which of the following standard Structured Streaming sink types are idempotent and can provide end-to-end exactly-once semantics in a Structured Streaming job?
A. Console
B. Kafka
C. File
D. Memory

Ans - B

Ques --
Which of following statements regarding caching are TRUE?
A. The default storage level for a DataFrame is StorageLevel.MEMORY_AND_DISK.
B. The uncache() method evicts a DataFrame from cache.
C. The persist() method immediately loads data from its source to materialize the DataFrame in cache.
D. Explicit caching can decrease application performance by interfering with the Catalyst optimizer's ability to optimize some queries.

Ans - A,B


==============================================================================================================================================

Practice exams 1
Practice exams

Which of the following is the deepest level in Spark‚Äôs execution hierarchy?
Ans: Task.

Explanation:
The hierarchy is, from top to bottom: Job, Stage, Task.
Executors and slots facilitate the execution of tasks, but they are not directly part of the
hierarchy. Executors are launched by the driver on worker nodes for the purpose of
running a specific Spark
application. Slots help Spark parallelize work. An executor can have multiple slots which
enable it to process multiple tasks in parallel.
Here is a great tip on how to differentiate actions from transformations: If an
operation returns a DataFrame, Dataset, or an RDD, it is a transformation.
Otherwise, it is an action. Actions do not send results to the driver, while
transformations do.


Q1: Which of the following describes a narrow transformation?
Ans: A narrow transformation is an operation in which no data is exchanged
across the cluster.


Q2: Which of the following statements about stages is correct?
Ans: Tasks in a stage may be executed by multiple machines at the same time.


Q3: Which of the following describes tasks?
Ans: Tasks get assigned to the executors by the driver.


Q4: Which of the following describes a difference between Spark's cluster and client
execution modes?
Ans: In cluster mode, the driver resides on a worker node, while it resides on an
edge node in client mode.


Q4: Which of the following describes Spark's standalone deployment mode?
Ans: Standalone mode uses only a single executor per worker per application.f


Practice exams 2


Q5: Which of the following describes properties of a shuffle?
Ans: In a shuffle, Spark writes data to disk.


Q6: Which of the following statements about the differences between actions and
transformations is correct?
Ans: Actions can trigger Adaptive Query Execution, while transformation cannot.


Q7: Which of the following is a characteristic of the cluster manager?
Ans: The cluster manager receives input from the driver through the  SparkContext.


Q8: Which of the following are valid execution modes?
Ans: Client, Cluster, Local


Q9: Deployment methods in spark
Ans: Deployment modes often refer to ways that Spark can be deployed in cluster mode
and how it uses specific frameworks outside Spark. Valid deployment modes are
standalone, Apache YARN, Apache Mesos and Kubernetes.


Q10: Which of the following describes Spark actions?
Ans: The driver receives data upon request by actions.


Q11: Which of the following statements about executors is correct?
Ans: Executors stop upon application completion by default.


Q12: Which of the following describes a valid concern about partitioning?
Ans: A shuffle operation returns 200 partitions if not explicitly set.


Q13: Which of the following is characteristic of using accumulators?
Ans: Accumulator values can only be read by the driver, but not by executors.


Q14: Which of the following statements about reducing out-of-memory errors is
incorrect?
Ans: Concatenating multiple string columns into a single column may guard
against out-of-memory errors


Q15: Which of the following statements about storage levels is incorrect?
Ans: MEMORY_AND_DISK  replicates cached DataFrames both on memory and disk.


Q16: Which of the following is not a feature of Adaptive Query Execution?
Practice exams 3
Ans: Reroute a query in case of an executor failure.


Q17: The code block displayed below contains an error. The code block should trigger
Spark to cache DataFrame  transactionsDf  in executor memory where available, writing
to disk where insufficient executor memory is available, in a fault-tolerant way. Find the
error.
Code block:
transactionsDf.persist(StorageLevel.MEMORY_AND_DISK)
Ans: The storage level is inappropriate for fault-tolerant storage.


Q18: The code block displayed below contains an error. The code block should
configure Spark to split data in 20 parts when exchanging data between executors for
joins or aggregations. Find the error.
Code block:
spark.conf.set(spark.sql.shuffle.partitions, 20)
Ans: Correct code block:
spark.conf.set("spark.sql.shuffle.partitions", 20)
The code block expresses the option incorrectly.
Correct! The option should be expressed as a string.


Q19: Column  predError  should be sorted by  desc_nulls_first()  instead.
Wrong. Since Spark's default sort order matches  asc_nulls_first() ,  null s would have
to come last when inverted.


Q20: While you might be tempted to change  unix_timestamp()  to  to_unixtime()  (in line
with the  from_unixtime()  operator), this function does not exist in
Spark.  unix_timestamp()  is the correct operator to use here.


Q21: itemsDf.withColumnRenamed(col("attributes"), col("feature0"), col("supplier"),
col("feature1"))
Wrong. The  DataFrame.withColumnRenamed()  operator takes exactly two string arguments.
So, in this answer both using  col()  and using four arguments is wrong.


Q22: itemsDf.withColumnRenamed("attributes", "feature0").withColumnRenamed("supplier",
"feature1")
Correct! Spark's  DataFrame.withColumnRenamed  syntax makes it relatively easy to change
the name of a column


Practice exams 4



Q23: 1. itemsDf.withColumnRenamed("attributes", "feature0")
2. itemsDf.withColumnRenamed("supplier", "feature1")
No. In this answer, the returned DataFrame will only have column  supplier  be renamed,
since the result of the first line is not written back to  itemsDf .


Q24: Find the error. Code
block: transactionsDf.write.partitionOn("storeId").parquet(filePath)
Ans: No method  partitionOn()  exists for the  DataFrame  class,  partitionBy()  should
be used instead.


Q25: Which of the following code blocks reads in the two-partition parquet file stored
at  filePath , making sure all columns are included exactly once even though each
partition has a different schema?
Ans: spark.read.option("mergeSchema", "true").parquet(filePath)
Correct. Spark's  DataFrameReader 's  mergeSchema  option will work well here, since columns
that appear in both partitions have matching data types. Note that  mergeSchema  would fail
if one or more columns with the same name that appear in both partitions would have
different data types.


Q26: On a side note: One answer option includes a function  str_split . This function
does not exist in pySpark. Only split exists.


‚Äî- ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî Line Break‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî Test 1‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî



Q1: Which of the following options describes the responsibility of the executors in
Spark?
Ans: The executors accept tasks from the driver, execute those tasks, and return
results to the driver.


Q2: Which of the following describes the role of tasks in the Spark execution hierarchy?
Ans: Tasks are the smallest element in the execution hierarchy


Q3: Which of the following describes the role of the cluster manager?
Ans: The cluster manager allocates resources to Spark applications and
maintains the executor processes in client mode.


Q4: Which of the following is the idea behind dynamic partition pruning in Spark?
Practice exams 5
Ans: Dynamic partition pruning is intended to skip over the data you do not need
in the results of a query.


Q5: Which of the following is one of the big performance advantages that Spark has
over Hadoop?
Ans: Spark achieves great performance by storing data and performing computation in
memory, whereas large jobs in Hadoop requrie a large amount of relatively slow disk I/O
operations.


Q6: Which of the following is the deepest level in Spark's execution hierarchy?
Ans: Task. The hierarchy is, from top to bottom: Job, Stage, Task.


Q7: Which of the following statements about garbage collection in Spark is incorrect?
Ans: Manually persisting RDDs in Spark prevents them from being garbage
collected.


Q8: Which of the following describes characteristics of the Dataset API?
Ans: The Dataset API is available in Scala, but it is not available in Python.


Q9: Which of the following describes the difference between client and cluster execution
modes?
Ans: In cluster mode, the driver runs on the worker nodes, while the client mode runs
the driver on the client machine.


Q10: Which of the following statements about executors is correct, assuming that one
can consider each of the JVMs working as executors as a pool of task execution slots?
Ans: Tasks run in parallel via slots.


Q11: Which of the following statements about RDDs is incorrect?
Ans: An RDD consists of a single partition.
Quite the opposite: Spark partitions RDDs and distributes the partitions across multiple
nodes.


Q12: Which of the following describes characteristics of the Spark UI?
Ans: There is a place in the Spark UI that shows the property  spark.executor.memory .


Q13: Which of the following statements about broadcast variables is correct?
Ans: Broadcast variables are immutable


Practice exams 6


Q14: Which of the following is a viable way to improve Spark's performance when
dealing with large amounts of data, given that there is only a single application running
on the cluster?
Ans: Increase values for the
properties  spark.default.parallelism  and  spark.sql.shuffle.partitions


Q15: Which of the following describes a shuffle?
Ans: A shuffle is a process that compares data between partitions


Q17: Which of the following describes Spark's Adaptive Query Execution?
Ans: Adaptive Query Execution features are dynamically switching join strategies
and dynamically optimizing skew joins.


Q18: Which of the following code blocks removes all rows in the 6-column
DataFrame  transactionsDf
 that have missing data in at least 3 columns?
Ans: transactionsDf.dropna(thresh=4)

Q19: transactionsDf.filter(col("storeId")==25).take(5)
Any of the options with  collect  will not work because  collect  does not take any
arguments, and in both cases the argument  5  is given.
Ans: transactionsDf.filter(col("storeId")==25).collect(5)


Q20: itemsDf.withColumnRenamed(col("manufacturer"), col("supplier"))
No. Watch out ‚Äì although the  col()  method works for many methods of the DataFrame
API,  withColumnRenamed  is not one of them. As outlined in the documentation linked
below,  withColumnRenamed  expects strings.


Q21: transactionsDf.desc_nulls_last("predError")
Wrong, this is invalid syntax. There is no method  DataFrame.desc_nulls_last()  in the
Spark API. There is a Spark function  desc_nulls_last()  however.
transactionsDf.orderBy("predError").asc_nulls_last()
Incorrect. There is no method  DataFrame.asc_nulls_last()  in the Spark API (see above).


Q22: Correct code block:
transactionsDf.drop("predError", "productId", "value")


Practice exams 7


The  select  operator should be replaced by the  drop  operator and the arguments
to the  drop  operator should be column names  predError ,  productId  and  value  as
strings.
Correct! It is important to know that the  drop  operator expects column names to be
passed as string arguments if multiple columns should be removed.


Q23: The code block displayed below contains an error. When the code block below
has executed, it should have divided DataFrame  transactionsDf  into 14 parts, based on
columns  storeId  and  transactionDate  (in this order). Find the error.
Code block:
transactionsDf.coalesce(14, ("storeId", "transactionDate"))
Ans: Correct code block:
transactionsDf.repartition(14, "storeId", "transactionDate").count()
In the Spark documentation, the call structure for  repartition  is shown like
this:  DataFrame.repartition(numPartitions, *cols) . The  *  operator means that any
argument after  numPartitions  will be interpreted as column. Therefore, the brackets
need to be removed.
Finally, the question specifies that after the execution the DataFrame should be divided.
So, indirectly this question is asking us to append an action to the code block.
Since  .select()  is a transformation. the only possible choice here is  .count() .


Q24: Correct code block: transactionsDf.select((col("storeId").between(20, 30)) &
(col("productId")==2))
Ans: Another riddle here is how to chain the two conditions. The only valid answer here
is  & . Operators like  &&  or  and  are not valid. Other boolean operators that would be
valid in Spark are  |  and  ~ .


Q25: Which of the following code blocks returns only rows from
DataFrame  transactionsDf  in which values in column  productId
 are unique?
Ans: transactionsDf.dropDuplicates(subset=[‚ÄùproductId‚Äù] In the documentation
for  dropDuplicates , the examples show that  subset
 should be used with a list.


Q26: The code block displayed below contains an error. The code block should
return a DataFrame where all entries in column  supplier  contain the letter


Practice exams 8


combination  et  in this order. Find the error.
Code block: itemsDf.filter(Column('supplier').isin('et'))
Ans: Correct code block: itemsDf.filter(col('supplier').contains('et'))


‚Äî- ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî Line Break‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî Test 2‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî


Q1: Which of the following statements about Spark's execution hierarchy is correct?
Ans: In Spark's execution hierarchy, a job may reach over multiple stage
boundaries.


Q2: Which of the following describes slots?
Ans: A Java Virtual Machine (JVM) working as an executor can be considered as a pool
of slots for task execution


Q3: Which of the following describes the conversion of a computational query into an
execution plan in Spark?
Ans: The executed physical plan depends on a cost optimization from a previous
stage.


Q4: Which of the following describes characteristics of the Spark driver?
Ans: The Spark driver‚Äôs responsibility includes scheduling queries for execution on
worker nodes


Q5: Which of the following describes executors?
Ans: Executors are responsible for carrying out work that they get assigned by
the driver.


Q6: Which of the following statements about DAGs is correct?
Ans: DAGs can be decomposed into tasks that are executed in parallel.


Q7: Which of the following statements about lazy evaluation is incorrect?
Ans: Execution is triggered by transformations.


Q8: Which of the following describes how Spark achieves fault tolerance?
Ans: If an executor on a worker node fails while calculating an RDD, that RDD can
be recomputed by another executor using the lineage.


Q9: Which of the following describes Spark's way of managing memory?
Ans: Storage memory is used for caching partitions derived from DataFrames.


Practice exams 9


Q10: Which of the following statements about Spark's DataFrames is incorrect?
Ans: Spark's DataFrames are equal to Python's or R's DataFrames.
Incorrect. They are only similar. A major difference between Spark and Python is that
Spark's DataFrames are distributed, whereby Python's are not.


Q11: Which of the following statements about Spark's configuration properties is
incorrect?
Ans: The default number of partitions to use when shuffling data for joins or
aggregations is 300.
No, the default value of the applicable property  spark.sql.shuffle.partitions  is 200.


Q12: Which of the following describes a way for resizing a DataFrame from 16 to 8
partitions in the most efficient way?
Ans: Use a narrow transformation to reduce the number of partitions.
Correct!  DataFrame.coalesce(n)  is a narrow transformation, and in fact the most efficient
way to resize the DataFrame of all options listed. One would run  DataFrame.coalesce(8)  to
resize the DataFrame.


Q13: Dataframe.select() is a narrow transformation


Q14: Which of the following statements about data skew is incorrect?
Ans: To mitigate skew, Spark automatically disregards null values in keys when
joining.


Q15: Which of the following describes the characteristics of accumulators?
Ans: If an action including an accumulator fails during execution and Spark
manages to restart the action and complete it successfully, only the successful
attempt will be counted in the accumulator.
Correct, when Spark tries to rerun a failed action that includes an accumulator, it will
only update the accumulator if the action succeeded.


Q16: Which of the following code blocks stores a part of the data in
DataFrame  itemsDf  on executors?
Ans: itemsDf.cache().count()
Caching means storing a copy of a partition on an executor, so it can be accessed
quicker by subsequent operations, instead of having to be recalculated.  cache()  is a
lazily-evaluated method of the DataFrame. Since  count()  is an action (while  filter()  is
not), it triggers the caching process.

Practice exams 10



Q17: This question targets your knowledge about how to chain filtering conditions. Each
filtering condition should be in parentheses. The correct operator for "or" is the pipe
character ( | ) and not the word  or . Another operator of concern is the equality
operator. For the purpose of comparison, equality is expressed as two equal signs ( == ).


Q18: The code block displayed below contains an error. The code block should produce
a DataFrame with  color  as the only column and three rows with  color  values
of  red ,  blue , and  green , respectively. Find the error.
Code block: 1. spark.createDataFrame([("red",), ("blue",), ("green",)], "color")
Ans: The  "color"  expression needs to be wrapped in brackets, so it
reads  ["color"] .(Correct)
Correct code block:
spark.createDataFrame([("red",), ("blue",), ("green",)], ["color"])
The  createDataFrame  syntax is not exactly straightforward, but luckily the documentation
(linked below) provides several examples on how to use it.


Q19: Which of the following code blocks stores DataFrame  itemsDf  in executor
memory and, if insufficient memory is available, serializes it and saves it to disk?
‚Ä¢ itemsDf.persist(StorageLevel.MEMORY_ONLY)
‚Ä¢ itemsDf.cache(StorageLevel.MEMORY_AND_DISK) (Incorrect)
‚Ä¢ itemsDf.store()
‚Ä¢ itemsDf.cache() (Correct)
‚Ä¢ itemsDf.write.option('destination', 'memory').save()
Ans: The key to solving this question is knowing (or reading in the documentation) that,
by default,  cache()  stores values to memory and writes any partitions for which there is
insufficient memory to disk.  persist()  can achieve the exact same behavior, however
not with the  StorageLevel.MEMORY_ONLY  option listed here. It is also worth noting
that  cache()  does not have any arguments.


Q20: Which of the following code blocks creates a new one-column, two-row
DataFrame  dfDates  with column  date  of type  timestamp ?
Ans:
1. dfDates = spark.createDataFrame([("23/01/2022 11:28:12",),("24/01/2022 10:58:34",)],
["date"])
Practice exams 11
2. dfDates = dfDates.withColumn("date", to_timestamp("date", "dd/MM/yyyy HH:mm:ss"))
(Correct)
When no schema is specified, Spark sets the  string  data type as default. So, the next
line is needed.


Q21: The code block displayed below contains an error. The code block should
configure Spark so that DataFrames up to a size of 20 MB will be broadcast to all
worker nodes when performing a join. Find the error.
Code block:
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 20)
Ans: Spark will only broadcast DataFrames that are much smaller than the default
value.
This is correct. The default value is 10 MB (10485760 bytes). Since the configuration
for  spark.sql.autoBroadcastJoinThreshold  expects a number in bytes (and not megabytes),
the code block sets the limits to merely 20 bytes, instead of the requested 20 * 1024 *
1024 (= 20971520) bytes.


Q22: Which of the following code blocks returns a DataFrame that is an inner join
of DataFrame  itemsDf  and DataFrame  transactionsDf , on
columns  itemId  and  productId , respectively and in which every  itemId  just
appears once?
Ans: itemsDf.join(transactionsDf,
itemsDf.itemId==transactionsDf.productId).dropDuplicates(["itemId"]) (Correct)
Filtering out distinct rows based on columns is achieved with the  dropDuplicates  method,
not the  distinct  method which does not take any arguments.


Q23: Which of the following code blocks generally causes a great amount of network
traffic?
Ans: DataFrame.collect()  sends all data in a DataFrame from executors to the driver, so
this generally causes a great amount of network traffic in comparison to the other
options listed.









===================================================================================

My Understanding --

## RDD Below are transformations ------------------------------------------------

map(func)
filter(func)
flatMap(func)
mapPartitions(func)
mapPartitionsWithIndex(func)
sample(withReplacement, fraction, seed)
union(otherDataset)
intersection(otherDataset)
distinct([numPartitions]))
groupByKey([numPartitions])
reduceByKey(func, [numPartitions])
aggregateByKey(zeroValue)(seqOp, combOp, [numPartitions])
sortByKey([ascending], [numPartitions])
join(otherDataset, [numPartitions])
cogroup(otherDataset, [numPartitions])
cartesian(otherDataset)
pipe(command, [envVars])
coalesce(numPartitions)
repartition(numPartitions)
repartitionAndSortWithinPartitions(partitioner)
***cache()
printSchema() select() limit() coalesce(numPartitions) toDF() toRDD()
sort()

## Below are Actions ------------------------------------------------------------

reduce(func)
collect()
count()
first()
take(n)
takeSample(withReplacement, num, [seed])
takeOrdered(n, [ordering])
saveAsTextFile(path)
saveAsSequenceFile(path) (Java and Scala)
saveAsObjectFile(path) (Java and Scala)
countByKey()
foreach(func)

# coalesce(numPartitions) vs repartition() --------------------------------

Returns a new DataFrame that has exactly numPartitions partitions.
coalesce uses existing partitions to minimize the amount of data that's shuffled.
repartition creates new partitions and does a full shuffle.
coalesce results in partitions with different amounts of data (sometimes partitions that have much different sizes)
and repartition results in roughly equal sized partitions.
Is coalesce or repartition faster?
coalesce may run faster than repartition,
but unequal sized partitions are generally slower to work with than equal sized partitions.
You'll usually need to repartition datasets after filtering a large data set.
I've found repartition to be faster overall because Spark is built to work with equal sized partitions.

narrow vs wide transformation ------------------

when the data needs to be moved (exchanged) across partitions then it is wide 

narrow --
select, filter, drop, withcolumn, coalesce 

wide --
join, groupBy, agg, repartition, sort









