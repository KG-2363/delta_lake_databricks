
# Display 
  shows data in tabular format
  limit upto 10000 records or upto 2 MB 


# Infer Schema vs Schema Enforcement

 when we use infer schema while creating a df,
  it initiates spark jobs 
  - forcing spark to infer schema 
 while when we provide a custom schema, it's just like a transformation. 
- did not invoke a spark job, 
  strongly typed datatypes, 
  used in production applications



# Partitions 
  Data divided into mutually exclusively in memory partitions 
  Size and number of Partitions affect parallelism
  Each partition processed independently 
  no of partitions = no of tasks 
  impacts performance and shuffle partitions



# Driver  -
  creates Spark session
  sends request to cluter manager for resources
  co ordinates tasks, handle failures


  
# Executors - run on worker nodes in a spark cluster and host tasks( for I/O and data processing)
   available CPU cores, memory resources, config settings
   Interact with driver for task coordination and data transfer
   store intermediate ad final results in memory or disk.


 
# Stages are group of tasks that can be executed in parallel.


# Principles of Distributed Computing -
Shared Nothing Architecture  -  Independence, Fault Tolerance, Scalability, Resource Partitioning
  Each node works independently and manages its own resources
  Tasks within a stage run in parallel known as shared nothing mode



# Clusters in Databricks Platform -
  All Purpose Clusters - interactive clusters that support notebooks, jobs, dashboards etc. with configurable auto termination.
  Job Clusters - ephemeral clusters that starts when a job runs and terminate automatically upon job completion.
  SQL Warehouses - Optimised clusters for sql query optimization with instant start up and auto scaling to balance cost and performance.



# Spark UI 
 Application UI - per application (Spark Session)
  app progress, task execution, dag visualisation, stage details
 Master UI - per cluster worker node status, health and cluster wide resources allocation



# Shuffling -
It is data re distribution across the clusters to perform aggregation operations such as group by, join, sorting.
needed when data needs to be combined across partitions

Wide transformations
Key based operations
data repartitioning

Number of partitions impact performance, overhead and needs to be minimised 

Optimizations
co located data 
avoid unnecessary shuffles
effective partitioning strategies 

# SparkSession table VS DataFrame Reader Method --

spark.table(name: str)
----------------------
Where: Method on SparkSession.
What it loads: Tables and views (including temporary/global temp views) registered in the session/catalog.
Options support: No—you can’t add .option(...) or .schema(...) because it directly materializes the table/view.


df = spark.table("sales.daily_orders")
df = spark.table("my_temp_view")          # works for session temp views
df = spark.table("global_temp.some_view") # global temp views

spark.read.table(name: str)
---------------------------


Where: Method on DataFrameReader (spark.read).
What it loads: Tables registered in the catalog; in practice, this also works for views, but it’s primarily designed for data-source-backed tables.
Options support: Yes—you can chain reader options before .table(...). Whether they’re honored depends on the underlying data source (e.g., Delta, Parquet).


df = spark.read.option("timeZone", "UTC").table("sales.daily_orders")
df = spark.read.table("sales.daily_orders")  # plain read

------------------------------------------------------------------------------------------------------------------------------------------------------------

# Map Shuffle Reduce Model
Each transformation goes through this simple model.


# Handle null methods
isNull() / isNotNull()
df.fillna() / df.na.fill()
df.dropna() / df.na.drop()


# Referencing df objects 
df.select("name")
df.select(df.first_name)
df.select(df["name"])
df.select(col("name").alias("customer_name")) -col when cast(), alias(), desc(), asc()


col("name").alias("customer_name")
col("age").cast(Integertype()) -- CHANGE DATA TYPE
col("age").isNotNull() -- CHECK FOR NULLS
col("title").contains("Manager") -- STRING MATCHING
df.sort(col("c1").desc(),col("c2").asc())

----------------------------------------------------------
 # sum(when(col("Year").isNull(),1).otherwise(0)).alias("Null_Year_count") -- Explain

col("Year").isNull()
Checks each row: True if Year is NULL, otherwise False.

when(..., 1).otherwise(0)
Converts that boolean to an integer flag:

1 for rows where Year is NULL
0 for rows where Year is not NULL

sum(...)
Adds up those 1s and 0s across the group (or the entire DataFrame if not grouped), resulting in the count of NULL


# FLIGHTS DATA INGESTION 
=========================================================================================================================================

flights_df = spark.table("dbacademy_airlines.vo1.flights_small")

flights_df.printSchema()

display(flights_df.limit(10))

flights_req_cols_df = flights_df.select(
 "Year",
 "Month",
 "Day Of Month",
 "DepTime",
 "FlightNum",
 .
 .
 .)

flights_count = flights_req_cols_df.count()

print(f"Source data has {flights_count} records")

# Temp View 
-- available in SQL Namespace 

flights_req_cols_df\
.selectExpr(
 "Year",
 "Month",
 "Day Of Month",
 "CAST(DepTime AS INT) as DepTime",
 "FlightNum"
 .
 .
 .
)\
.createOrReplaceTempView("flights_temp")


# Invalid counts sql using Spark SQl 

invalid_counts_sql = spark.sql("""
SELECT 
COUNTIF(Year IS NULL) as null_year_count,
COUNTIF(Month IS NULL) as month_null_count,
.
.
.
FROM 
flights_temp
""")

display(invalid_counts_sql)

# display invalid counts using dataframe API 

from pyspark.sql.functions import sum, col, when

flights_temp_df = spark.table("flights_temp")

invalid_counts_df = flights_temp_df.select(
 sum(when(col("Year").isNull(),1).otherwise(0)).alias("Null_Year_count"),
 sum(when(col("Month").isNull(),1).otherwise(0)).alias("Null_Month_Count"),
 sum(when(col("Day Of Month").isNull(),1).otherwise(0)).alias("Null_Day_Of_Month_Count"),
 .
 .
 .
)

display(invalid_counts_df)

-- Both operations have same output as per optimizations 
sql_plan = invalid_counts_sql.explain()

df_plan = invalid_counts_df.explain()

sql_plan == df_plan # True



# DATA CLEANING
=========================================================================================================================

-- remove null data

non_null_flights_df = flights_req_cols_df.na.drop(
how = 'any',
subset = ['CRSElapsedTime']
)

-------------------------------------------------

flights_req_cols_df.na.drop(...)
Uses the DataFrameNaFunctions API to drop rows with NULL (or NaN) values.

Parameters explained:

how='any' → Drop the row if any of the specified columns in subset are NULL.
(Alternative: how='all' → Drop only if all are NULL.)
subset=['CRSElapsedTime'] → Only check the column CRSElapsedTime for nulls.
So effectively: remove rows where CRSElapsedTime is NULL.

non_null_flights_df will contain all rows from flights_req_cols_df except those where CRSElapsedTime is NULL.
Alternative --
non_null_flights_df = flights_req_cols_df.filter(F.col("CRSElapsedTime").isNotNull())


--------------------------------------------------------------------------------------------------------------


-- This will remove all the data of these cols that can't be converted into INT datatype.


from pyspark.sql.functions import col

flights_valid_data_df = non_null_flights_df.filter(
  col("Arrdelay").cast("INTEGER").isNotNull() &
  col("ActualElapsedTime").cast("INTEGER").isNotNull() &
  col("DepTime").cast("INTEGER").isNotnull()
)


clean_flights_df = flights_valid_data_df\
  .withColumn("ArrDelay", col("arrDelay").cast("integer"))\
  .withColumn("ActualElapsedTime",col("actualElapsedTime").cast("integer"))



clean_flights_df.printSchema()


# DATA ENRICHMENT 
=============================================================================================================================================

# Derive flight Datetime column

from pyspark.sql.functions import col, make_timestamp_ntz, lpad, substring, lit

flights_datetime_df = clean_flights_df.withColumn(
  "FlightDateTime", 
  make_timestamp_ntz(
  col("Year"),
  col("Month"),
  col("Day Of Month"),
  substr(lpad(col("Deptime"),4,"0"), lit(1), lit(2)).cast("integer"),
  substr(lpad(col("DepTime"),4,"0"), lit(3), lit(2)).cast("integer"),
  lit(0)
).drop("Year","Month","Day Of Month","DepTime")

display(flights_datetime_df).limit(10)


# Derive Elapsed time Diff 

from pyspark.sql.functions import col

flights_elapsed_time_diff_df = flights_datetime_df.withColumn("Elapsed_Time_Diff",
col("Actual_Elapsed_Time_Diff") - col("CRSElapsed_Time_Diff")).drop("Actual_Elapsed_Time_Diff","CRSElapsed_Time_Diff")

display(flights_elapsed_time_diff_df).limit(10)

# Categorize Arrdelay column into OnTime, SlightDelay, ModerateDelay, SevereDelay

sql -- we use case when statement
pyspark -- when 
or switch statement in other languages

from pyspark.sql.functions import when 

enriched_flights_df = flights_elapsed_time_diff_df \
        .withColumn("delay_category", when(col(Arrdelay) <= 0,"On_Time")
                                      .when(col(Arrdelay <= 15, "Slight_Delay")
                                      .when(col(Arrdelay <= 60, "Moderate_Delay")
                                      .otherwise("Severe_Delay"))
         .drop("Arrdelay")
display(enriched_flights_df.limit(10))



# Working with UDFs

from pyspark.sql.functions import pandas_udf 

@pandas_udf("double")
def normalized_diff(diff_series):
   return ((diff_series - diff_series.mean()) / diff_series.std() 

udf_example = enriched_flights_df\
                .withColumn("diff_normalized",normalized_diff("ElapsedTimeDiff"))

display(udf_example)


================================================================================================================

BASIC GROUPING 
------------------

trips_df = spark.read.table("samples.nyctaxi.trips")

location_count = trips_df\
                  .grouBy("pickup_zip")\
                  .count()\
                  .orderBy(desc("count"))

display(location_counts.limit(10))


location_stats = trips_df\
                   .groupBy("pickup_zip")\
                   .agg(
                     count("*").alias("total_trips"),
                     round(avg("trip_distance"),2).alias("avg_distance"),
                     round(avg("fare_amount"),2).alias("avg_fare"),
                     round(sum("fare_amount"),2).alias("total_fare_amount")
                   .orderBy(desc("total_trips"))

display(location_stats.limit(5))


-------------------------------------------------------------------------------------
WINDOW FUNCTIONS

from pyspark.sql.functions import Window

window_by_trips = Window.orderBy(desc("total_trips"))
window_by_fare = Window.orderBy(desc("avg_fare"))


ranked_locations = location_stats\
               .withColumn("trips_rank",rank().over(window_by_trips))\
               .withColumn("fare_rank",rank().over(window_by_fare))\
               .withColumn("fare_quintile", ntile(5).over(window_by_fare)) # divide into five groups by fare

ranked_locations.createOrReplaceTempView("ranked_locations")

------------------------------------------------------------------------------------

joins --

df1.join(df2, "user_id") -- inner by default

df1.join(df2, on=df1.user_id == df2.user_id, how ="inner")

users.alias("u").join(orders.alias("o"), [col("u.id") == col("o.user_id"), col("u.region") == col("o.region")])

df1.unionAll(df2) - to preserve duplicates

-------------------------------------------------------------------------------------

# join strategy 
small table should be referenced first

small_df.join(large_df,"key")

large_df.join(small_df,"key") -- less efficient

# broadcast - avoids heavy shuffles

from pyspark.sql.functions import broadcast

large_df.join(broadcast(small_df),"key")

------------------------------------------------------------------------------------

enriched_transactions = franchises_df\
                       .select(
                       "franchiseID",
                        col("name").alias("store_name"),
                        col("city").alias("store_city"),
                        col("country").alias("store_country")
                        )    
                       .join(transactions_df, on="key",how="inner")



# full Outer Join helps to identify the missing relationships or null records across datasets.


full_join = franchises_df\
            .withColumnRenamed("name","franchise_name")\
            .join(
             suppliers_df.select("supplierID",col("name").alias("supplier_name")),
             on = "key",
             how = "full_outer"
             )

# supplier ID which is not associated with a franchise --

non_matching_records = full_join.filter(col("franchiseID").isNull() | col("supplierID").iNull())\
                         .select("franchiseID","franchisename",col("supplierID").alias("orphaned_supplier_id"))


================================================================================================================

USING SPARK SQL 

franchises_df.createOrReplaceTempView("franchises")
suppliers_df.createOrReplaceTempView("suppliers")

%sql
SELECT f.franchiseId,
f.name as franchise_name,
f.supplierID as orphaned_supplier_id
FROM franchises f
FULL OUTER JOIN suplliers s
ON f.supplierID = s.supplierID
WHERE f.supplierID is null or s.name is null


-----------------------------------------------------------------------

COMPLEX DATA TYPES IN SPARK --


Arrays, maps and structs  -- Native to Spark, execution is faster

Struct types maintain a logical grouping of related fields like a table 
            accesed using dot notation or getField() column method.

functions for creating collections - collect_list, collect_set - Uses in aggregation pipelines

manipulate and transform array type columns using functions - explode, array_contains and element_at

# JSON STRINGS vs STRUCTS

A json string is just a string with data formatted in JSON.

To Spark, it is just a string untill it's parsed.

Struct is spark native and hence doesn't need parsing saving memory and computation during transformation.

Convert early using from_json and schema 

Example 
id, json_data
1, { "user_id": 100,"features":{"score":0.8}}

schema = StructType([ StructField("user_id",LongType()), StructField("features",MapType(stringType(), DoubleType())) ])

df2 = df1.withColumn("parsed", from_json(col("json_data"),schema))

EXPLODE Function --
Unnesting nested data in arrays while preserving other columns 
One row per array element
id, items 
1, ["a","b","c"]

df.select("id", explode("items").alias("item")

Result -
1,a
1,b
1,c


array_contains(col,val) -- tests if array conatins specific value 
e.g. array_contains(items, "a")

size(col) -- returns number of elements 
e.g. size(items)

element_at(col,n) -- returns nth element (1 based indexing)
e.g.  element_at(items,2)

array_distinct(col) -- removes duplicate from array columns 
e.g. array_distinct(items)


# Aggregating to Collections --
grouping items into array 

collect_list() -- aggregate all values from a columns to an array
                  commonly used with groupBy to build arrays of related values within each group

e.g  df.groupBy("region")\
       .agg(
            collect_list("product").alias("all_products"),
            collect_set("product").alias("distinct_products")
           )

# Hands on example datasets ---

dataset --

raw_user_df = spark.read.table("raw_user_data")

raw_user_df.printSchema()

user_id  string
name  string 
active  string
signup_date  string
interests  string  ["hiking", "ML", "Photography"]
recent_purchases  string [{"product_id" : 101, "name":"ANSH"...}


Interests is an array of strings, using predefined schema


interests_schema = ArrayType(StringType())


recent_purchases_json = raw_user_data_df.select("recent_purchases").limit(1).collect()[0][0]


recent_purchases_schema = schema_of_json(lit(recent_purchases_json))



parsed_users_df = raw_user_data_df.select(
    col("user_id").cast("integer")
    col("name"),
    col("active").cast("boolean"),
    col("signup_date").cast("date"),
    from_json(col("interests"), interests_schema).alias("interests"),
    from_json(col("recent_purchases"),recent_purchases_schema).alias("recent_purchases")
)


interests array - element string 
purchases array - element struct 



display(
   parsed_users_df.select(
  "user_id",
   array_size("interests").alias("number_of_interests"),
   array_size("purchases").alias("number_of_purchases")
   )
)


user_101s_interests_df = parsed_users_df.select("user_id","interests").filter(parsed_users_df.user_id == 101)
display(user_101s_interests_df)



display(
    user_101s_interests_df.select(
       "user_id",
        explode("interests").alias("interest")
      )
)


exploded_df = parsed_users_df.select("user_id", explode("interests").alias("interest"))


user_interests_df = exploded_df.groupBy("user_id").agg(collect_list("interest).alias("interests"))


------------------------------------------------------------------------------------------------------------


Referencing structs fields 































       
   



















































                     


                 










                
              



 
















