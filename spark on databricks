# Spark --

A cluster, or group of machines, pools the resources of many machines together allowing us to use all the cumulative
resources as if they were one. Now a group of machines alone is not powerful, you need a framework to coordinate
work across them. Spark is a tool for just that, managing and coordinating the execution of tasks on data across a
cluster of computers.


A cluster, or group of machines, pools the resources of many machines together allowing us to use all the cumulative
resources as if they were one. Now a group of machines alone is not powerful, you need a framework to coordinate
work across them. Spark is a tool for just that, managing and coordinating the execution of tasks on data across a
cluster of computers.


The cluster of machines that Spark will leverage to execute tasks will be managed by a cluster manager like Spark’s
Standalone cluster manager, YARN, or Mesos. We then submit Spark Applications to these cluster managers which will
grant resources to our application so that we can complete our work.


# Display 
  shows data in tabular format
  limit upto 10000 records or upto 2 MB 


# Infer Schema vs Schema Enforcement

 when we use infer schema while creating a df,
  it initiates spark jobs 
  - forcing spark to infer schema 
 while when we provide a custom schema, it's just like a transformation. 
- did not invoke a spark job, 
  strongly typed datatypes, 
  used in production applications



# Partitions 
  Data divided into mutually exclusively in memory partitions 
  Size and number of Partitions affect parallelism
  Each partition processed independently 
  no of partitions = no of tasks 
  impacts performance and shuffle partitions


Spark breaks up the data into chunks, called partitions. A
partition is a collection of rows that sit on one physical machine in our cluster. A DataFrame’s partitions represent how
the data is physically distributed across your cluster of machines during execution. 


# Driver  -  It’s the heart of a Spark Application and maintains all relevant information during the lifetime of the application.
  The driver process, sits on a node in the cluster and is responsible for three things: 
     maintaining information about the Spark application;
     responding to a user’s program;
     and analyzing, distributing, and scheduling work across the executors. 

  Driver process compute the resource requirement, manages end to end lifecycle of a job, including dynamic increase/ decrease of resource computation, schedule and 
  distribute the work across spark executors, manages node failures by requesting alternative resources, keep track of executor progress, interact with program input.


# Executors - run on worker nodes in a spark cluster and host tasks( for I/O and data processing)
   The executors are responsible for only two things: 
      1. executing code assigned to it by the driver and
      2. reporting the state of the computation, on that executor, back to the driver node.
   available CPU cores, memory resources, config settings
   Interact with driver for task coordination and data transfer
   store intermediate ad final results in memory or disk.


# Cluster Managers -- The cluster manager controls physical machines and allocates resources to Spark applications. This can be one of several core cluster managers: 
Spark’s standalone cluster manager, YARN, or Mesos. This means that there can be multiple Spark applications running on a cluster at the same time. 

  YARN
  Mesos
  Standalone 

# Modes 
  Local    -- In local mode, these run (as threads) on your individual computer instead of a cluster
  Client   -- 
  Cluster  --


# query plans --
“sort”, “exchange”, and “FileScan”. 

df.explain()

== Physical Plan ==
TakeOrderedAndProject(limit=5, orderBy=[destination_total#16194L DESC],
output=[DEST_COUNTRY_
+- *HashAggregate(keys=[DEST_COUNTRY_NAME#7323], functions=[sum(count#7325L)])
 +- Exchange hashpartitioning(DEST_COUNTRY_NAME#7323, 5)
 +- *HashAggregate(keys=[DEST_COUNTRY_NAME#7323], functions=[partial_
sum(count#7325L)])
 +- InMemoryTableScan [DEST_COUNTRY_NAME#7323, count#7325L]
 +- InMemoryRelation [DEST_COUNTRY_NAME#7323, ORIGIN_COUNTRY_NAME#7324, count#
 +- *Scan csv [DEST_COUNTRY_NAME#7578,ORIGIN_COUNTRY_NAME#7

 -- see how our aggregation happens in two phases, in the partial_sum calls. This is because summing a list of numbers is commutative and Spark can perform the sum,
partition by partition.


# The DataFrame concept is not unique to Spark. R and Python both have similar concepts. However, Python/R DataFrames (with some exceptions)
exist on one machine rather than multiple machines.
This limits what you can do with a given DataFrame in python and R to the resources that exist on that specific machine.

# Row objects --
  A logical record or row is an object of type Row. Row
  objects are the objects that column expressions operate on to produce some usable value. Row objects represent
  physical byte arrays. The byte array interface is never shown to users because we only use column expressions to
  manipulate them.

 from pyspark.sql import Row
 myRow = Row(“Hello”, None, 1, False)

# withColumnRenamed -- to use the withColumnRenamed method. This will rename the column with the name of the string in the first argument, to the
string in the second argument.

df.withColumnRenamed(“DEST_COUNTRY_NAME”, “dest”).columns

# desc function -- You might also notice that desc
does not return a string but a Column.

# drop Columns --
df.drop(“ORIGIN_COUNTRY_NAME”).columns
We can drop multiple columns by passing in multiple columns as arguments.
dfWithLongColName.drop(“ORIGIN_COUNTRY_NAME”, “DEST_COUNTRY_NAME”)


 # Slots - In Apache Spark, a slot is a unit of execution that is used to execute tasks on a worker node. 
A slot is essentially a thread of execution that is managed by the Spark executor.
When you submit a Spark job, the Spark executor will allocate a number of slots on each worker node in the cluster.
The number of slots allocated is determined by the spark.executor.cores configuration property, 
which specifies the number of CPU cores that each executor should use. 
By default, each executor will use all the available CPU cores on a worker node, 
but this can be configured to a smaller value if desired.

Tasks in a Spark job are executed in parallel by multiple slots, with each task being assigned to a slot for execution. 
This allows Spark to make efficient use of the available CPU resources on a cluster and to parallelize the execution of tasks.
Slots can be either "on-demand" or "reserved". On-demand slots are allocated to a Spark job as needed, 
while reserved slots are allocated to a job in advance and are not released until the job completes. 
By default, Spark uses on-demand slots, but you can configure your cluster to use reserved slots instead by setting the spark.
dynamicAllocation.enabled configuration property to false.
Slots are not the same thing as executors. Executors could have multiple slots in them, and tasks are executed on slots.

-------------------------------------------------------------------------------------------------------------------------------

# What are accumulators?
Why use Spark Accumulators?


Accumulators are variables that are used for aggregating information across the executors.
For example, this information can pertain to data or API diagnosis like how many records are corrupted or how many times a particular library API was called.

To understand why we need accumulators, let’s see a small example.


Here’s an imaginary log of transactions of a chain of stores around the central Kolkata region.


logs-Spark-accumulators
There are 4 fields,

Field 1 -> City

Field 2 -> Locality

Field 3 -> Category of item sold

Field 4 -> Value of item sold

However, the logs can be corrupted. For example, the second line is a blank line, the fourth line reports some network issues and 
finally the last line shows a sales value of zero (which cannot happen!).

We can use accumulators to analyse the transaction log to find out the number of blank logs (blank lines),
number of times the network failed, any product that does not have a category or even number of times zero sales were recorded. The full sample log can be found here.


Accumulators are applicable to any operation which are,
1. Commutative -> f(x, y) = f(y, x), and
2. Associative -> f(f(x, y), z) = f(f(x, z), y) = f(f(y, z), x)
For example, sum and max functions satisfy the above conditions whereas average does not.

Why use Spark Accumulators?
Now why do we need accumulators and why not just use variables as shown in the code below.

variables-spark-accumulators

The problem with the above code is that when the driver prints the variable blankLines its value will be zero. This is because when Spark ships this code to every executor the variables become local to that executor and its updated value is not relayed back to the driver. To avoid this problem we need to make blankLines an accumulator such that all the updates to this variable in every executor is relayed back to the driver.
So the above code should be written as,
code-spark-accumulators
This guarantees that the accumulator blankLines is updated across every executor and the updates are relayed back to the driver.

We can implement other counters for network errors or zero sales value, etc. The full source code along with the implementation of the other counters can be found here.

People familiar with Hadoop Map-Reduce will notice that Spark’s accumulators are similar to Hadoop’s Map-Reduce counters.


When using accumulators there are some caveats that we as programmers need to be aware of, ---

Computations inside transformations are evaluated lazily, so unless an action happens on an RDD the transformationsare not executed. As a result of this, accumulators used inside functions like map() or filter() wont get executed unless some action happen on the RDD.
Spark guarantees to update accumulators inside actionsonly once. So even if a task is restarted and the lineage is recomputed, the accumulators will be updated only once.
Spark does not guarantee this for transformations. So if a task is restarted and the lineage is recomputed, there are chances of undesirable side effects when the accumulators will be updated more than once.
To be on the safe side, always use accumulators inside actions ONLY.

------------------------------------------------------------------------------------------------------------------------------------

# Stages are group of tasks that can be executed in parallel.


# Principles of Distributed Computing -
Shared Nothing Architecture  -  Independence, Fault Tolerance, Scalability, Resource Partitioning
  Each node works independently and manages its own resources
  Tasks within a stage run in parallel known as shared nothing mode



# Clusters in Databricks Platform -
  All Purpose Clusters - interactive clusters that support notebooks, jobs, dashboards etc. with configurable auto termination.
  Job Clusters - ephemeral clusters that starts when a job runs and terminate automatically upon job completion.
  SQL Warehouses - Optimised clusters for sql query optimization with instant start up and auto scaling to balance cost and performance.



# Spark UI 
 Application UI - per application (Spark Session)
  app progress, task execution, dag visualisation, stage details
 Master UI - per cluster worker node status, health and cluster wide resources allocation



# Shuffling -
It is data re distribution across the clusters to perform aggregation operations such as group by, join, sorting.
needed when data needs to be combined across partitions


Narrow transformation -- Transformations consisting of narrow dependencies are those where each input partition will contribute to only one output partition. 
Our where clause specifies a narrow dependency, where only one partition contributes to at most one output partition.

# Pipelining on narrow dependencies, this means that if we specify multiple filters on DataFrames they’ll all be performed in memory. The
same cannot be said for shuffles. When we perform a shuffle, Spark will write the results to disk. 

Wide transformations -- A wide dependency style transformation will have input partitions contributing to many output partitions. We call this a shuffle where Spark
will exchange partitions across the cluster. 


Key based operations
data repartitioning

Number of partitions impact performance, overhead and needs to be minimised 

Optimizations
co located data 
avoid unnecessary shuffles
effective partitioning strategies 

# SparkSession table VS DataFrame Reader Method --

spark.table(name: str)
----------------------
Where: Method on SparkSession.
What it loads: Tables and views (including temporary/global temp views) registered in the session/catalog.
Options support: No—you can’t add .option(...) or .schema(...) because it directly materializes the table/view.


df = spark.table("sales.daily_orders")
df = spark.table("my_temp_view")          # works for session temp views
df = spark.table("global_temp.some_view") # global temp views

spark.read.table(name: str)
---------------------------


Where: Method on DataFrameReader (spark.read).
What it loads: Tables registered in the catalog; in practice, this also works for views, but it’s primarily designed for data-source-backed tables.
Options support: Yes—you can chain reader options before .table(...). Whether they’re honored depends on the underlying data source (e.g., Delta, Parquet).


df = spark.read.option("timeZone", "UTC").table("sales.daily_orders")
df = spark.read.table("sales.daily_orders")  # plain read

------------------------------------------------------------------------------------------------------------------------------

df.write.mode("overwrite").option("compression", "snappy").save("path")
Parquet is the default file format. If you don’t include the format() method, the DataFrame will still be saved as a Parquet file.

And if the file name already exist in the path given and if you don't include option mode("overwrite")  you will get an error.

------------------------------------------------------------------------------------------------------------------------------------------------------------
df.filter(col("count") < 2)
df.where("count < 2")

------------------------------------------------------------------------------------------------------------------------------
# Map Shuffle Reduce Model
Each transformation goes through this simple model.


##  Handle null methods ------------------------------------------------------------------------------------------------------------------
isNull() / isNotNull()
df.fillna() / df.na.fill()
df.dropna() / df.na.drop()

# Drop
The simplest is probably drop, which simply removes rows that contain nulls. The default is to drop any row where
any value is null.
df.na.drop()
df.na.drop(“any”)
df.na.drop("all")

We can also apply this to certain sets of columns by passing in an array of columns.
%scala
df.na.drop(“all”, Seq(“StockCode”, “InvoiceNo”))
%python
df.na.drop(“all”, subset=[“StockCode”, “InvoiceNo”])

Passing in “any” as an argument will drop a row if any of the values are null. Passing in “all” will only drop the row if all
values are null or NaN for that row.

We can also apply this to certain sets of columns by passing in an array of columns.
%scala
df.na.drop(“all”, Seq(“StockCode”, “InvoiceNo”))
%python
df.na.drop(“all”, subset=[“StockCode”, “InvoiceNo”])

# Fill -------------------------------------------------------------------------------------------------------------------------------

Fill allows you to fill one or more columns with a set of values. This can be done by specifying a map, specific value
and a set of columns.

For example to fill all null values in String columns I might specify.

df.na.fill(“All Null values become this string”)

We could do the same for integer columns with df.na.fill(5:Integer) or for Doubles df.na.
fill(5:Double). In order to specify columns, we just pass in an array of column names like we did above.
%scala
df.na.fill(5, Seq(“StockCode”, “InvoiceNo”))
%python
df.na.fill(“all”, subset=[“StockCode”, “InvoiceNo”])
We can also do with with a Scala Map where the key is the column name and the value is the value we would like to use to fill null values.

ill_cols_vals = {
“StockCode”: 5,
“Description” : “No Value”
}
df.na.fill(fill_cols_vals)

Probably the most common use case is to replace all values in a certain column
according to their current value. The only requirement is that this value be the same type as the original value.

df.na.replace([“”], [“UNKNOWN”], “Description”)


## UDFs -------------------------------------------------------------------------------------------------------------------------------------------

User defined functions or UDFs and can take and return one or more columns as input. Spark UDFs are incredibly powerful because they can be written in several different
programming languages and do not have to be written in an esoteric format or DSL. They’re just functions that operate on the data, record by record.

%scala
val udfExampleDF = spark.range(5).toDF(“num”)
def power3(number:Double):Double = {
 number * number * number
}
power3(2.0)
%python
udfExampleDF = spark.range(5).toDF(“num”)
def power3(double_value):
return double_value ** 3
power3(2.0)

Now that we’ve created these functions and tested them, we need to register them with Spark so that we can used them on all of our worker machines.
Spark will serialize the function on the driver and transfer it over the network to all executor processes. This happens regardless of language.

Once we go to use the function, there are essentially two different things that occur. --

If the function is written in Scala or Java then we can use that function within the JVM. This means there will be little performance penalty aside from the fact that 
we can’t take advantage of code generation capabilities that Spark has for built-in functions. 
There can be performance issues if you create or use a lot of objects which we will cover in the optimization section.

If the function is written in Python, something quite different happens. Spark will start up a python process on the worker, serialize all of the data to a format that
python can understand (remember it was in the JVM before), execute the function row by row on that data in the python process, before finally returning the results of 
the row operations to the JVM and Spark.

WARNING: Starting up this Python process is expensive but the real cost is in serializing the data to Python. This is costly for two reasons, it is an expensive computation
but also once the data enters Python, Spark cannot manage the memory of the worker. This means that you could potentially cause a worker to fail if it becomes resource
constrained (because both the JVM and python are competing for memory on the same machine). 

We recommend that you write your UDFs in Scala - the small amount of time it should take you to write the function in Scala will always yield significant speed ups and 
on top of that, you can still use the function from Python!



# Referencing df objects 
df.select("name")
df.select(df.first_name)
df.select(df["name"])
df.select(col("name").alias("customer_name")) -col when cast(), alias(), desc(), asc()


col("name").alias("customer_name")
col("age").cast(Integertype()) -- CHANGE DATA TYPE
col("age").isNotNull() -- CHECK FOR NULLS
col("title").contains("Manager") -- STRING MATCHING
df.sort(col("c1").desc(),col("c2").asc())



----------------------------------------------------------

The fourth step is a simple renaming, we use the withColumnRenamed method that takes two arguments, the
original column name and the new column name. Of course, this doesn’t perform computation - this is just another
transformation! 

----------------------------------------------------------
 # sum(when(col("Year").isNull(),1).otherwise(0)).alias("Null_Year_count") -- Explain

col("Year").isNull()
Checks each row: True if Year is NULL, otherwise False.

when(..., 1).otherwise(0)
Converts that boolean to an integer flag:

1 for rows where Year is NULL
0 for rows where Year is not NULL

sum(...)
Adds up those 1s and 0s across the group (or the entire DataFrame if not grouped), resulting in the count of NULL


# FLIGHTS DATA INGESTION 
=========================================================================================================================================

flights_df = spark.table("dbacademy_airlines.vo1.flights_small")

flights_df.printSchema()

display(flights_df.limit(10))

flights_req_cols_df = flights_df.select(
 "Year",
 "Month",
 "Day Of Month",
 "DepTime",
 "FlightNum",
 .
 .
 .)

flights_count = flights_req_cols_df.count()

print(f"Source data has {flights_count} records")

# Temp View 
-- available in SQL Namespace 

flights_req_cols_df\
.selectExpr(
 "Year",
 "Month",
 "Day Of Month",
 "CAST(DepTime AS INT) as DepTime",
 "FlightNum"
 .
 .
 .
)\
.createOrReplaceTempView("flights_temp")


# Invalid counts sql using Spark SQl 

invalid_counts_sql = spark.sql("""
SELECT 
COUNTIF(Year IS NULL) as null_year_count,
COUNTIF(Month IS NULL) as month_null_count,
.
.
.
FROM 
flights_temp
""")

display(invalid_counts_sql)

# display invalid counts using dataframe API 

from pyspark.sql.functions import sum, col, when

flights_temp_df = spark.table("flights_temp")

invalid_counts_df = flights_temp_df.select(
 sum(when(col("Year").isNull(),1).otherwise(0)).alias("Null_Year_count"),
 sum(when(col("Month").isNull(),1).otherwise(0)).alias("Null_Month_Count"),
 sum(when(col("Day Of Month").isNull(),1).otherwise(0)).alias("Null_Day_Of_Month_Count"),
 .
 .
 .
)

display(invalid_counts_df)

-- Both operations have same output as per optimizations 
sql_plan = invalid_counts_sql.explain()

df_plan = invalid_counts_df.explain()

sql_plan == df_plan # True



# DATA CLEANING
=========================================================================================================================

-- remove null data

non_null_flights_df = flights_req_cols_df.na.drop(
how = 'any',
subset = ['CRSElapsedTime']
)

-------------------------------------------------

flights_req_cols_df.na.drop(...)
Uses the DataFrameNaFunctions API to drop rows with NULL (or NaN) values.

Parameters explained:

how='any' → Drop the row if any of the specified columns in subset are NULL.
(Alternative: how='all' → Drop only if all are NULL.)
subset=['CRSElapsedTime'] → Only check the column CRSElapsedTime for nulls.
So effectively: remove rows where CRSElapsedTime is NULL.

non_null_flights_df will contain all rows from flights_req_cols_df except those where CRSElapsedTime is NULL.
Alternative --
non_null_flights_df = flights_req_cols_df.filter(F.col("CRSElapsedTime").isNotNull())


--------------------------------------------------------------------------------------------------------------


-- This will remove all the data of these cols that can't be converted into INT datatype.


from pyspark.sql.functions import col

flights_valid_data_df = non_null_flights_df.filter(
  col("Arrdelay").cast("INTEGER").isNotNull() &
  col("ActualElapsedTime").cast("INTEGER").isNotNull() &
  col("DepTime").cast("INTEGER").isNotnull()
)


clean_flights_df = flights_valid_data_df\
  .withColumn("ArrDelay", col("arrDelay").cast("integer"))\
  .withColumn("ActualElapsedTime",col("actualElapsedTime").cast("integer"))



clean_flights_df.printSchema()


# DATA ENRICHMENT 
=============================================================================================================================================

# Derive flight Datetime column

from pyspark.sql.functions import col, make_timestamp_ntz, lpad, substring, lit

flights_datetime_df = clean_flights_df.withColumn(
  "FlightDateTime", 
  make_timestamp_ntz(
  col("Year"),
  col("Month"),
  col("Day Of Month"),
  substr(lpad(col("Deptime"),4,"0"), lit(1), lit(2)).cast("integer"),
  substr(lpad(col("DepTime"),4,"0"), lit(3), lit(2)).cast("integer"),
  lit(0)
).drop("Year","Month","Day Of Month","DepTime")

display(flights_datetime_df).limit(10)


# Derive Elapsed time Diff 

from pyspark.sql.functions import col

flights_elapsed_time_diff_df = flights_datetime_df.withColumn("Elapsed_Time_Diff",
col("Actual_Elapsed_Time_Diff") - col("CRSElapsed_Time_Diff")).drop("Actual_Elapsed_Time_Diff","CRSElapsed_Time_Diff")

display(flights_elapsed_time_diff_df).limit(10)

# Categorize Arrdelay column into OnTime, SlightDelay, ModerateDelay, SevereDelay

sql -- we use case when statement
pyspark -- when 
or switch statement in other languages

from pyspark.sql.functions import when 

enriched_flights_df = flights_elapsed_time_diff_df \
        .withColumn("delay_category", when(col(Arrdelay) <= 0,"On_Time")
                                      .when(col(Arrdelay <= 15, "Slight_Delay")
                                      .when(col(Arrdelay <= 60, "Moderate_Delay")
                                      .otherwise("Severe_Delay"))
         .drop("Arrdelay")
display(enriched_flights_df.limit(10))



# Working with UDFs

from pyspark.sql.functions import pandas_udf 

@pandas_udf("double")
def normalized_diff(diff_series):
   return ((diff_series - diff_series.mean()) / diff_series.std() 

udf_example = enriched_flights_df\
                .withColumn("diff_normalized",normalized_diff("ElapsedTimeDiff"))

display(udf_example)


================================================================================================================

BASIC GROUPING 
------------------

trips_df = spark.read.table("samples.nyctaxi.trips")

location_count = trips_df\
                  .grouBy("pickup_zip")\
                  .count()\
                  .orderBy(desc("count"))

display(location_counts.limit(10))


location_stats = trips_df\
                   .groupBy("pickup_zip")\
                   .agg(
                     count("*").alias("total_trips"),
                     round(avg("trip_distance"),2).alias("avg_distance"),
                     round(avg("fare_amount"),2).alias("avg_fare"),
                     round(sum("fare_amount"),2).alias("total_fare_amount")
                   .orderBy(desc("total_trips"))

display(location_stats.limit(5))


-------------------------------------------------------------------------------------
WINDOW FUNCTIONS

from pyspark.sql.functions import Window

window_by_trips = Window.orderBy(desc("total_trips"))
window_by_fare = Window.orderBy(desc("avg_fare"))


ranked_locations = location_stats\
               .withColumn("trips_rank",rank().over(window_by_trips))\
               .withColumn("fare_rank",rank().over(window_by_fare))\
               .withColumn("fare_quintile", ntile(5).over(window_by_fare)) # divide into five groups by fare

ranked_locations.createOrReplaceTempView("ranked_locations")

------------------------------------------------------------------------------------

joins --

df1.join(df2, "user_id") -- inner by default

df1.join(df2, on=df1.user_id == df2.user_id, how ="inner")

users.alias("u").join(orders.alias("o"), [col("u.id") == col("o.user_id"), col("u.region") == col("o.region")])

df1.unionAll(df2) - to preserve duplicates

-------------------------------------------------------------------------------------

# join strategy 
small table should be referenced first

small_df.join(large_df,"key")

large_df.join(small_df,"key") -- less efficient

# broadcast - avoids heavy shuffles

from pyspark.sql.functions import broadcast

large_df.join(broadcast(small_df),"key")

------------------------------------------------------------------------------------

enriched_transactions = franchises_df\
                       .select(
                       "franchiseID",
                        col("name").alias("store_name"),
                        col("city").alias("store_city"),
                        col("country").alias("store_country")
                        )    
                       .join(transactions_df, on="key",how="inner")



# full Outer Join helps to identify the missing relationships or null records across datasets.


full_join = franchises_df\
            .withColumnRenamed("name","franchise_name")\
            .join(
             suppliers_df.select("supplierID",col("name").alias("supplier_name")),
             on = "key",
             how = "full_outer"
             )

# supplier ID which is not associated with a franchise --

non_matching_records = full_join.filter(col("franchiseID").isNull() | col("supplierID").iNull())\
                         .select("franchiseID","franchisename",col("supplierID").alias("orphaned_supplier_id"))


================================================================================================================

USING SPARK SQL 

franchises_df.createOrReplaceTempView("franchises")
suppliers_df.createOrReplaceTempView("suppliers")

%sql
SELECT f.franchiseId,
f.name as franchise_name,
f.supplierID as orphaned_supplier_id
FROM franchises f
FULL OUTER JOIN suplliers s
ON f.supplierID = s.supplierID
WHERE f.supplierID is null or s.name is null


-----------------------------------------------------------------------

COMPLEX DATA TYPES IN SPARK --


Arrays, maps and structs  -- Native to Spark, execution is faster

Struct types maintain a logical grouping of related fields like a table 
            accesed using dot notation or getField() column method.

functions for creating collections - collect_list, collect_set - Uses in aggregation pipelines

manipulate and transform array type columns using functions - explode, array_contains and element_at

# JSON STRINGS vs STRUCTS

A json string is just a string with data formatted in JSON.

To Spark, it is just a string untill it's parsed.

Struct is spark native and hence doesn't need parsing saving memory and computation during transformation.

Convert early using from_json and schema 

Example 
id, json_data
1, { "user_id": 100,"features":{"score":0.8}}

schema = StructType([ StructField("user_id",LongType()), StructField("features",MapType(stringType(), DoubleType())) ])

df2 = df1.withColumn("parsed", from_json(col("json_data"),schema))

EXPLODE Function --
Unnesting nested data in arrays while preserving other columns 
One row per array element
id, items 
1, ["a","b","c"]

df.select("id", explode("items").alias("item")

Result -
1,a
1,b
1,c


array_contains(col,val) -- tests if array conatins specific value 
e.g. array_contains(items, "a")

size(col) -- returns number of elements 
e.g. size(items)

element_at(col,n) -- returns nth element (1 based indexing)
e.g.  element_at(items,2)

array_distinct(col) -- removes duplicate from array columns 
e.g. array_distinct(items)


# Aggregating to Collections --
grouping items into array 

collect_list() -- aggregate all values from a columns to an array
                  commonly used with groupBy to build arrays of related values within each group

e.g  df.groupBy("region")\
       .agg(
            collect_list("product").alias("all_products"),
            collect_set("product").alias("distinct_products")
           )

# Hands on example datasets ---

dataset --


raw_user_df = spark.read.table("raw_user_data")


raw_user_df.printSchema()


user_id  string
name  string 
active  string
signup_date  string
interests  string  ["hiking", "ML", "Photography"]
recent_purchases  string [{"product_id" : 101, "name":"ANSH"...}


Interests is an array of strings, using predefined schema


interests_schema = ArrayType(StringType())


recent_purchases_json = raw_user_data_df.select("recent_purchases").limit(1).collect()[0][0]


recent_purchases_schema = schema_of_json(lit(recent_purchases_json))



parsed_users_df = raw_user_data_df.select(
    col("user_id").cast("integer")
    col("name"),
    col("active").cast("boolean"),
    col("signup_date").cast("date"),
    from_json(col("interests"), interests_schema).alias("interests"),
    from_json(col("recent_purchases"),recent_purchases_schema).alias("recent_purchases")
)


interests array - element string 
purchases array - element struct 



display(
   parsed_users_df.select(
  "user_id",
   array_size("interests").alias("number_of_interests"),
   array_size("purchases").alias("number_of_purchases")
   )
)


user_101s_interests_df = parsed_users_df.select("user_id","interests").filter(parsed_users_df.user_id == 101)
display(user_101s_interests_df)



display(
    user_101s_interests_df.select(
       "user_id",
        explode("interests").alias("interest")
      )
)


exploded_df = parsed_users_df.select("user_id", explode("interests").alias("interest"))


user_interests_df = exploded_df.groupBy("user_id").agg(collect_list("interest).alias("interests"))


------------------------------------------------------------------------------------------------------------


Referencing structs fields 



exploded_purchases_df = raw_user_df.select("user_id"),explode("recent_purchases").alias("purchase"))

display(exploded_purchases_df)

recent_purchases_df = exploded_purchases_df.select(
        "user_id",
         col("purchase.date").alias("purchase_date"),
         col("purchase.product_id").alias("product_id"),
         col("purchase.price").alias("purchase_price"),
         col("purchase.name").alias("purchase_name")
     )

display("recent_purchases_df)



field_access_df = exploded_purchases_df.select(
        "user_id",
         col("purchase").getField("date").alias("purchase_date"),
         col("purchase").getField("product_id").alias("product_id"),
         col("purchase").getField("price").alias("product_price"),
         col("purchase").getField("name").alias("purchase_name")
     )



Pivot method -- It is used to transform row data into columnar format, creating a cross tabulation.



pivot_df = (recent_purchases_df
             .groupBy("user_id"),
             .pivot("product_name"),
             .agg(count("product_id").alias("quantity_purchases"))
             .fillna(0)
       )

--------------------------------------------------------------------------------------------------------------------------------------------------------------
## Working with Dates and Timestamps ---
Dates and times are a constant challenge in programming languages and databases. It’s always necessary to keep track of timezones and make sure that formats are 
correct and valid. Spark does its best to keep things simple by focusing explicitly on two kinds of time related information. 
There are dates, which focus exclusively on calendar dates, and timestamps that include both date and time information.


Now as we hinted at above, working with dates and timestamps closely relates to working with strings because we often store our timestamps or dates as strings and
convert them into date types at runtime. This is less common when working with databases and structured data but much more common when we are working with text and csv files.

Now Spark, as we saw with our current dataset, will make a best effort to correctly identify column types, including
dates and timestamps when we enable inferSchema. We can see that this worked quite well with our current
dataset because it was able to identify and read our date format without us having to provide some specification
for it.
df.printSchema()

%python
from pyspark.sql.functions import current_date, current_timestamp
dateDF = spark.range(10)\
.withColumn(“today”, current_date())\
.withColumn(“now”, current_timestamp())
dateDF.createOrReplaceTempView(“dateTable”)
dateDF.printSchema()

# date_sub, date_add functions ----------
Now that we have a simple DataFrame to work with, let’s add and subtract 5 days from today. These functions take a column and then the number of days to either add or subtract as the arguments.

%python
from pyspark.sql.functions import date_add, date_sub

dateDF\
.select(
date_sub(col(“today”), 5),
date_add(col(“today”), 5))\
.show(1)

%sql
SELECT
date_sub(today, 5),
date_add(today, 5)
FROM
dateTable


# datediff, months_between  function -----------------------------------------------------------------------------

datediff function that will return the number of days in between two dates. 
months_between that gives you the number of months between two dates.


%python

from pyspark.sql.functions import datediff, months_between, to_date
dateDF\
.withColumn(“week_ago”, date_sub(col(“today”), 7))\
.select(datediff(col(“week_ago”), col(“today”)))\
.show(1)


dateDF\
.select(
to_date(lit(“2016-01-01”)).alias(“start”),
to_date(lit(“2017-05-22”)).alias(“end”))\
.select(months_between(col(“start”), col(“end”)))\
.show(1)

%sql
SELECT the to_date function. This function allows us to convert a date
of the format “2017-01-01” to a Spark date. Of course, for this to work our date must be in the year-month-day
format. 

# to_date(‘2016-01-01’),
months_between(‘2016-01-01’, ‘2017-01-01’),
datediff(‘2016-01-01’, ‘2017-01-01’)
FROM
dateTable

from pyspark.sql.functions import to_date, lit
spark.range(5).withColumn(“date”, lit(“2017-01-01”))\
.select(to_date(col(“date”)))\
.show(1)


WARNING
Spark will not throw an error if it cannot parse the date, it’ll just return null. This can be a bit tricky in larger
pipelines because you may be expecting your data in one format and getting it in another. To illustrate, let’s take
a look at the date format that has switched from year-month-day to year-day-month. Spark will fail to parse this
date and silently return null instead.


dateDF.select(to_date(lit(“2016-20-12”)),to_date(lit(“2017-12-11”))).show(1)
to_date(2016-20-12)    to_date(2017-12-11)
null                   2017-12-11


%python

from pyspark.sql.functions import unix_timestamp, from_unixtime

dateFormat = “yyyy-dd-MM”

cleanDateDF = spark.range(1)\
.select(
to_date(unix_timestamp(lit(“2017-12-11”), dateFormat).cast(“timestamp”))\
.alias(“date”),
to_date(unix_timestamp(lit(“2017-20-12”), dateFormat).cast(“timestamp”))\
.alias(“date2”))

cleanDateDF.createOrReplaceTempView(“dateTable2”)


%sql
SELECT
to_date(cast(unix_timestamp(date, ‘yyyy-dd-MM’) as timestamp)),
to_date(cast(unix_timestamp(date2, ‘yyyy-dd-MM’) as timestamp)),
to_date(date)
FROM
dateTable2


cleanDateDF\
.select(
unix_timestamp(col(“date”), dateFormat).cast(“timestamp”))\
.show()

Once we’ve gotten our date or timestamp into the correct format and type,Comparing between them is actually quite
easy. We just need to be sure to either use a date/timestamp type or specify our string according to the right format of
yyyy-MM-dd if we’re comparing a date.

cleanDateDF.filter(col(“date2”) > lit(“2017-12-12”)).show()

One minor point is that we can also set this as a string which Spark parses to a literal.


             















                   































       
   



















































                     


                 










                
              



 
















