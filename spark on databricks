
# Display 
  shows data in tabular format
  limit upto 10000 records or upto 2 MB 


# Infer Schema vs Schema Enforcement

 when we use infer schema while creating a df,
  it initiates spark jobs 
  - forcing spark to infer schema 
 while when we provide a custom schema, it's just like a transformation. 
- did not invoke a spark job, 
  strongly typed datatypes, 
  used in production applications



# Partitions 
  Data divided into mutually exclusively in memory partitions 
  Size and number of Partitions affect parallelism
  Each partition processed independently 
  no of partitions = no of tasks 
  impacts performance and shuffle partitions



# Driver  -
  creates Spark session
  sends request to cluter manager for resources
  co ordinates tasks, handle failures


  
# Executors - run on worker nodes in a spark cluster and host tasks( for I/O and data processing)
   available CPU cores, memory resources, config settings
   Interact with driver for task coordination and data transfer
   store intermediate ad final results in memory or disk.


 
# Stages are group of tasks that can be executed in parallel.


# Principles of Distributed Computing -
Shared Nothing Architecture  -  Independence, Fault Tolerance, Scalability, Resource Partitioning
  Each node works independently and manages its own resources
  Tasks within a stage run in parallel known as shared nothing mode



# Clusters in Databricks Platform -
  All Purpose Clusters - interactive clusters that support notebooks, jobs, dashboards etc. with configurable auto termination.
  Job Clusters - ephemeral clusters that starts when a job runs and terminate automatically upon job completion.
  SQL Warehouses - Optimised clusters for sql query optimization with instant start up and auto scaling to balance cost and performance.



# Spark UI 
 Application UI - per application (Spark Session)
  app progress, task execution, dag visualisation, stage details
 Master UI - per cluster worker node status, health and cluster wide resources allocation



# Shuffling -
It is data re distribution across the clusters to perform aggregation operations such as group by, join, sorting.
needed when data needs to be combined across partitions

Wide transformations
Key based operations
data repartitioning

Number of partitions impact performance, overhead and needs to be minimised 

Optimizations
co located data 
avoid unnecessary shuffles
effective partitioning strategies 

# SparkSession table VS DataFrame Reader Method --

spark.table(name: str)
----------------------
Where: Method on SparkSession.
What it loads: Tables and views (including temporary/global temp views) registered in the session/catalog.
Options support: No—you can’t add .option(...) or .schema(...) because it directly materializes the table/view.


df = spark.table("sales.daily_orders")
df = spark.table("my_temp_view")          # works for session temp views
df = spark.table("global_temp.some_view") # global temp views

spark.read.table(name: str)
---------------------------


Where: Method on DataFrameReader (spark.read).
What it loads: Tables registered in the catalog; in practice, this also works for views, but it’s primarily designed for data-source-backed tables.
Options support: Yes—you can chain reader options before .table(...). Whether they’re honored depends on the underlying data source (e.g., Delta, Parquet).


df = spark.read.option("timeZone", "UTC").table("sales.daily_orders")
df = spark.read.table("sales.daily_orders")  # plain read

------------------------------------------------------------------------------------------------------------------------------------------------------------

# Map Shuffle Reduce Model
Each transformation goes through this simple model.


# Handle null methods
isNull() / isNotNull()
df.fillna() / df.na.fill()
df.dropna() / df.na.drop()


# Referencing df objects 
df.select("name")
df.select(df.first_name)
df.select(df["name"])
df.select(col("name").alias("customer_name")) -col when cast(), alias(), desc(), asc()


col("name").alias("customer_name")
col("age").cast(Integertype()) -- CHANGE DATA TYPE
col("age").isNotNull() -- CHECK FOR NULLS
col("title").contains("Manager") -- STRING MATCHING
df.sort(col("c1").desc(),col("c2").asc())

----------------------------------------------------------
 # sum(when(col("Year").isNull(),1).otherwise(0)).alias("Null_Year_count") -- Explain

col("Year").isNull()
Checks each row: True if Year is NULL, otherwise False.

when(..., 1).otherwise(0)
Converts that boolean to an integer flag:

1 for rows where Year is NULL
0 for rows where Year is not NULL

sum(...)
Adds up those 1s and 0s across the group (or the entire DataFrame if not grouped), resulting in the count of NULL


# FLIGHTS DATA INGESTION 
=========================================================================================================================================

flights_df = spark.table("dbacademy_airlines.vo1.flights_small")

flights_df.printSchema()

display(flights_df.limit(10))

flights_req_cols_df = flights_df.select(
 "Year",
 "Month",
 "Day Of Month",
 "DepTime",
 "FlightNum",
 .
 .
 .)

flights_count = flights_req_cols_df.count()

print(f"Source data has {flights_count} records")

# Temp View 
-- available in SQL Namespace 

flights_req_cols_df\
.selectExpr(
 "Year",
 "Month",
 "Day Of Month",
 "CAST(DepTime AS INT) as DepTime",
 "FlightNum"
 .
 .
 .
)\
.createOrReplaceTempView("flights_temp")


# Invalid counts sql using Spark SQl 

invalid_counts_sql = spark.sql("""
SELECT 
COUNTIF(Year IS NULL) as null_year_count,
COUNTIF(Month IS NULL) as month_null_count,
.
.
.
FROM 
flights_temp
""")

display(invalid_counts_sql)

# display invalid counts using dataframe API 

from pyspark.sql.functions import sum, col, when

flights_temp_df = spark.table("flights_temp")

invalid_counts_df = flights_temp_df.select(
 sum(when(col("Year").isNull(),1).otherwise(0)).alias("Null_Year_count"),
 sum(when(col("Month").isNull(),1).otherwise(0)).alias("Null_Month_Count"),
 sum(when(col("Day Of Month").isNull(),1).otherwise(0)).alias("Null_Day_Of_Month_Count"),
 .
 .
 .
)

display(invalid_counts_df)

-- Both operations have same output as per optimizations 
sql_plan = invalid_counts_sql.explain()

df_plan = invalid_counts_df.explain()

sql_plan == df_plan # True



# DATA CLEANING
=========================================================================================================================

-- remove null data

non_null_flights_df = flights_req_cols_df.na.drop(
how = 'any',
subset = ['CRSElapsedTime']
)

-------------------------------------------------

flights_req_cols_df.na.drop(...)
Uses the DataFrameNaFunctions API to drop rows with NULL (or NaN) values.

Parameters explained:

how='any' → Drop the row if any of the specified columns in subset are NULL.
(Alternative: how='all' → Drop only if all are NULL.)
subset=['CRSElapsedTime'] → Only check the column CRSElapsedTime for nulls.
So effectively: remove rows where CRSElapsedTime is NULL.

non_null_flights_df will contain all rows from flights_req_cols_df except those where CRSElapsedTime is NULL.
Alternative --
non_null_flights_df = flights_req_cols_df.filter(F.col("CRSElapsedTime").isNotNull())


--------------------------------------------------------------------------------------------------------------


-- This will remove all the data of these cols that can't be converted into INT datatype.


from pyspark.sql.functions import col

flights_valid_data_df = non_null_flights_df.filter(
  col("Arrdelay").cast("INTEGER").isNotNull() &
  col("ActualElapsedTime").cast("INTEGER").isNotNull() &
  col("DepTime").cast("INTEGER").isNotnull()
)


clean_flights_df = flights_valid_data_df\
  .withColumn("ArrDelay", col("arrDelay").cast("integer"))\
  .withColumn("ActualElapsedTime",col("actualElapsedTime").cast("integer"))



clean_flights_df.printSchema()


# DATA ENRICHMENT 
=============================================================================================================================================

# Derive flight Datetime column

from pyspark.sql.functions import col, make_timestamp_ntz, lpad, substring, lit

flights_datetime_df = clean_flights_df.withColumn(
  "FlightDateTime", 
  make_timestamp_ntz(
  col("Year"),
  col("Month"),
  col("Day Of Month"),
  substr(lpad(col("Deptime"),4,"0"), lit(1), lit(2)).cast("integer"),
  substr(lpad(col("DepTime"),4,"0"), lit(3), lit(2)).cast("integer"),
  lit(0)
).drop("Year","Month","Day Of Month","DepTime")

display(flights_datetime_df).limit(10)


# Derive Elapsed time Diff 

from pyspark.sql.functions import col

flights_elapsed_time_diff_df = flights_datetime_df.withColumn("Elapsed_Time_Diff",
col("Actual_Elapsed_Time_Diff") - col("CRSElapsed_Time_Diff")).drop("Actual_Elapsed_Time_Diff","CRSElapsed_Time_Diff")

display(flights_elapsed_time_diff_df).limit(10)

# Categorize Arrdelay column into OnTime, SlightDelay, ModerateDelay, SevereDelay





 
















