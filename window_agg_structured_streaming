STATEFUL VS STATELESS OPERATIONS --
-------------------------------------------------------------------


Stateless - processes each record independently e.g. SELECT, FILTER
Stateful  - maintain information across batches e.g. GROUPBY, JOIN


from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window


orders_schema = StructType([
    StructField("customer_id",LongType(), True),
    StructField("notifications",StringType(), True),
    StructField("order_id",LongType(), True),
    StructField("order_timestamp",LongType(), True)
 )]


status_schema = StructType([
    StructField("order_id",LongType(),True),
    StructField("order_status",StringType(),True),
    StructField("status_timestamp",LongType(),True)
 )]


orders_stream = spark.readStream\
            .format("json")\
            .schema(orders_schema)\
            .option("maxFilesPerTrigger",1)\
            .option("path","/Volumes..")
            .load()


status_stream = spark.readStream\
            .format("json")\
            .schema(status_schema)
            .option("maxFilesPerTrigger",1)\
            .load()

# stateless operation example --

orders_transformed = orders_stream\
            .withColumn("order_time",from_unixtime(col("order_timestamp).cast("timestamp"))
            .withColumn("notification_enabled",col("notifications")=="Y")


# stateful operation example --

status_counts = status_stream\
            .groupBy("order_status")
            .count()
            .orderBy(col("count").desc())



# Window Operations

Window operations allow us to perform aggregations over time windows. 

Tumbling Windows (fixed, non-overlapping)

Sliding Windows (overlapping windows)


1. Tumbling Window Example
----------------------------------------------------

Let's count orders per 1-minute tumbling window:

step1 - to clean up previous streams with same name -

for query in spark.streams.active:
    if query.name == "tumbling_window_counts":
        query.stop()

step2 - prepare data with proper timestamp column 

status_events = status_stream\
         .withColumn("event_time",from_unixtime(col("status_timestamp").cast("timestamp")))


tumbling_windows = status_events\
         .groupBy(
           window(col("event_time"),"1 minute"),
           col(order_status)
         )
      .count()


tumbling_window_query = tumbling_windows\
          .writeStream\
          .format("memory")\
          .outputMode("complete")\
          .queryName("tumbling_window_counts")\
          .start()


-- Query the tumbling window results
SELECT 
  window.start as window_start,
  window.end as window_end,
  order_status,
  count
FROM tumbling_window_counts
ORDER BY window_start, order_status



2. Sliding Window Example

Now let's count orders per 2-minute window, sliding every 1 minute:


for query in spark.streams.active:
    if query.name == "sliding_window_counts":
        query.stop()


sliding_windows = status_events\
         .groupBy(
            window(col("event_time"),"2 minutes","1 minute"),
            col("order_status"),
           )\
      .count()


sliding_window_query = sliding_windows.writeStream\ 
          .format("memory")\
          .outputMode("complete")\
          .queryName("sliding_window_counts")\
          .start()


%sql
-- Query the sliding window results
SELECT 
  window.start as window_start,
  window.end as window_end,
  order_status,
  count
FROM sliding_window_counts
ORDER BY window_start, order_status




D. Streaming Joins
Let's demonstrate joining our streaming order data with status updates.


# Prepare our streaming DataFrames with proper timestamps

orders_with_time = orders_stream \
    .withColumn("order_time", from_unixtime(col("order_timestamp")).cast("timestamp"))

status_with_time = status_stream \
    .withColumn("status_time", from_unixtime(col("status_timestamp")).cast("timestamp"))


1. Stream-Static Join

First, let's create a static DataFrame for lookup purposes.


# Create a static lookup table for order status descriptions

status_lookup = spark.createDataFrame([
    ("placed", "Order has been placed"),
    ("preparing", "Order is being prepared"),
    ("on the way", "Order is in transit"),
    ("delivered", "Order has been delivered"),
    ("cancelled", "Order has been cancelled")
], ["order_status", "status_description"])


# Join streaming status data with static status descriptions
enriched_status = status_with_time \
    .join(status_lookup, "order_status")

# Display the joined stream
display(enriched_status)



2. Stream-Stream Join
Now let's join our two streaming DataFrames.



# Stop any existing queries
for query in spark.streams.active:
    if query.name == "order_status_join":
        query.stop()

# Join order stream with status stream on order_id
# Note: We need to limit state buildup for production use
order_status_join = orders_with_time \
    .join(
        status_with_time,
        "order_id"
    )

# Write to memory sink
order_status_join_query = order_status_join.writeStream \
    .format("memory") \
    .outputMode("append") \
    .queryName("order_status_join") \
    .start()

%sql
-- Query the joined data
SELECT 
  order_id, 
  customer_id, 
  order_status,
  notifications,
  order_time,
  status_time
FROM order_status_join
LIMIT 20

E. Handling Late Data with Watermarks
---------------------------------------------------------------------------------------------------

Watermarks help us handle late-arriving data by defining how long to wait for late events.



# Stop any existing queries

for query in spark.streams.active:
    if query.name == "windowed_with_watermark":
        query.stop()

# Add watermark to status events

status_with_watermark = status_events \
    .withWatermark("event_time", "10 minutes")

# Windows with watermark

watermarked_windows = status_with_watermark \
    .groupBy(
        window(col("event_time"), "5 minutes"),
        col("order_status")
    ) \
    .count()

# Write to memory

query5 = watermarked_windows.writeStream \
    .format("memory") \
    .outputMode("complete") \
    .queryName("windowed_with_watermark") \
    .start()


%sql
-- Query the windowed data with watermark
SELECT 
  window.start as window_start,
  window.end as window_end,
  order_status,
  count
FROM windowed_with_watermark
ORDER BY window_start, order_status
























































            







         















        



































